#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•MovieDetailCrawlerçš„parse_movie_pageæ–¹æ³•
ä½¿ç”¨å·²ä¿å­˜çš„HTMLæ–‡ä»¶ç›´æ¥æµ‹è¯•è§£æåŠŸèƒ½
"""

import sys
import os
import json
import time
import re
from pathlib import Path
from bs4 import BeautifulSoup
from loguru import logger

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ç®€åŒ–ç‰ˆçš„MovieDetailCrawlerç±»ï¼ŒåªåŒ…å«parse_movie_pageæ–¹æ³•
class SimpleMovieDetailCrawler:
    """ç®€åŒ–ç‰ˆçš„ç”µå½±è¯¦æƒ…è§£æå™¨ï¼Œç”¨äºæµ‹è¯•parse_movie_pageæ–¹æ³•"""
    
    def __init__(self, movie_code: str):
        self.movie_code = movie_code
        self.field_patterns = self.initialize_field_patterns()
    
    def initialize_field_patterns(self):
        """åˆå§‹åŒ–å­—æ®µæ¨¡å¼"""
        return {
            "genre": ["ã‚¸ãƒ£ãƒ³ãƒ«", "é¡å‹", "Genre", "ç±»å‹"],
            "actress": ["å¥³å„ª", "æ¼”å“¡", "Actress", "å¥³ä¼˜"],
            "studio": ["ã‚¹ã‚¿ã‚¸ã‚ª", "å·¥ä½œå®¤", "Studio", "åˆ¶ä½œå•†"],
            "label": ["ãƒ¬ãƒ¼ãƒ™ãƒ«", "å» ç‰Œ", "Label", "å‚ç‰Œ"],
            "series": ["ã‚·ãƒªãƒ¼ã‚º", "ç³»åˆ—", "Series"],
            "director": ["ç›£ç£", "å°æ¼”", "Director", "å¯¼æ¼”"]
        }
    
    def extract_m3u8_info(self, html: str) -> dict:
        """æå–M3U8åŠ å¯†ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾åŒ…å«M3U8ä¿¡æ¯çš„scriptæ ‡ç­¾
            script_pattern = r'var\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*"([^"]+)"[^}]*var\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*=\s*\[([^\]]+)\]'
            match = re.search(script_pattern, html)
            
            if match:
                encrypted_code = match.group(2)
                dictionary_str = match.group(4)
                dictionary = [item.strip('"') for item in dictionary_str.split(',')]
                
                logger.info(f"æ‰¾åˆ°M3U8åŠ å¯†ä¿¡æ¯: ä»£ç é•¿åº¦={len(encrypted_code)}, å­—å…¸é•¿åº¦={len(dictionary)}")
                return {
                    "encrypted_code": encrypted_code,
                    "dictionary": dictionary
                }
            else:
                logger.warning("æœªæ‰¾åˆ°M3U8åŠ å¯†ä¿¡æ¯")
                return {"encrypted_code": "", "dictionary": []}
        except Exception as e:
            logger.error(f"æå–M3U8ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return {"encrypted_code": "", "dictionary": []}
    
    def deobfuscate_m3u8(self, encrypted_code: str, dictionary: list) -> list:
        """è§£å¯†M3U8 URL"""
        if not encrypted_code or not dictionary:
            logger.warning("è§£å¯†M3U8å¤±è´¥: åŠ å¯†ä»£ç æˆ–å­—å…¸ä¸ºç©º")
            return []
        
        try:
            results = []
            decoded = ""
            
            for c in encrypted_code:
                if c in [".", "-", "/", ":"]:
                    decoded += c
                else:
                    try:
                        number = int(c, 16)
                        if 0 <= number < len(dictionary):
                            decoded += dictionary[number]
                    except ValueError:
                        decoded += c
            
            if decoded and (".m3u8" in decoded or "/master" in decoded):
                results.append(decoded)
                logger.info(f"è§£å¯†å‡ºM3U8 URL: {decoded[:50]}...")
            
            return results
        except Exception as e:
            logger.error(f"è§£å¯†M3U8æ—¶å‡ºé”™: {e}")
            return []
    
    def _extract_info_by_label(self, soup, label_text: str) -> list:
        """æ ¹æ®æ ‡ç­¾æ–‡æœ¬æå–ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾åŒ…å«æ ‡ç­¾æ–‡æœ¬çš„å…ƒç´ 
            label_elem = soup.find(text=re.compile(label_text, re.IGNORECASE))
            if label_elem:
                parent = label_elem.parent
                # æŸ¥æ‰¾ç›¸é‚»çš„é“¾æ¥å…ƒç´ 
                links = parent.find_next_siblings('a') or parent.find_all('a')
                return [link.get_text(strip=True) for link in links if link.get_text(strip=True)]
        except Exception:
            pass
        return []
    
    def _extract_single_field_by_label(self, soup, label_text: str) -> str:
        """æ ¹æ®æ ‡ç­¾æ–‡æœ¬æå–å•ä¸ªå­—æ®µ"""
        try:
            info_list = self._extract_info_by_label(soup, label_text)
            return info_list[0] if info_list else ""
        except Exception:
            return ""
    
    def parse_movie_page(self, html: str) -> dict:
        """åˆ†æç”µå½±é¡µé¢HTMLä»¥æå–è¯¦ç»†ä¿¡æ¯"""
        result = {
            "id": self.movie_code,
            "url": f"https://missav.ai/ja/{self.movie_code}",
            "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            "title": "",
            "cover_url": "",
            "release_date": "",
            "duration_seconds": None,
            "studio": "",
            "label": "",
            "series": "",
            "director": "",
            "tags": [],
            "actresses": [],
            "description": "",
            "magnets": [],
            "m3u8_urls": [],
        }
        
        try:
            # æ£€æŸ¥HTMLå†…å®¹çš„æœ‰æ•ˆæ€§
            if not html or len(html) < 1000:
                logger.error(f"HTMLå†…å®¹é•¿åº¦ä¸è¶³: {len(html) if html else 0}")
                return result

            # éªŒè¯HTMLæ˜¯å¦åŒ…å«ç”µå½±ID
            # if self.movie_code.upper() not in html.upper():
            #     logger.error(f"HTMLå†…å®¹ä¸åŒ…å«ç”µå½±ID: {self.movie_code}")
            #     return result

            # éªŒè¯ä¸æ˜¯é¦–é¡µ
            if "MissAV | ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§ç„¡æ–™" in html and not re.search(
                r"<h1[^>]*>[^<]*" + re.escape(self.movie_code) + r"[^<]*</h1>",
                html,
                re.IGNORECASE,
            ):
                logger.error(f"HTMLå†…å®¹å¯èƒ½æ˜¯ç½‘ç«™é¦–é¡µ")
                return result

            soup = BeautifulSoup(html, "html.parser")

            # å°è¯•æå–é¡µé¢æ ‡é¢˜
            # 1. å…ˆä»h1æ ‡ç­¾è·å–
            h1_title = soup.select_one("h1")
            if (
                h1_title
                and len(h1_title.text) > 3
                and self.movie_code.upper() in h1_title.text.upper()
            ):
                result["title"] = h1_title.text.strip()
                logger.info(f"ä»h1æ ‡ç­¾è·å–åˆ°æ ‡é¢˜: {result['title']}")

            # 2. å°è¯•ä»metaæ ‡ç­¾è·å–
            if not result["title"] or "MissAV" in result["title"]:
                og_title = soup.select_one('meta[property="og:title"]')
                if og_title:
                    title_text = og_title.get("content", "").strip()
                    if self.movie_code.upper() in title_text.upper():
                        result["title"] = title_text
                        logger.info(f"ä»og:titleè·å–åˆ°æ ‡é¢˜: {result['title']}")

            # 3. å¦‚æœè¿˜æ˜¯æ²¡æœ‰è·å–åˆ°ï¼Œå°è¯•ä»titleæ ‡ç­¾è·å–
            if not result["title"] or "MissAV" in result["title"]:
                title_tag = soup.select_one("title")
                if title_tag and self.movie_code.upper() in title_tag.text.upper():
                    result["title"] = title_tag.text.strip()
                    logger.info(f"ä»titleæ ‡ç­¾è·å–åˆ°æ ‡é¢˜: {result['title']}")

            # è·å–å°é¢å›¾ç‰‡
            # 1. å…ˆå°è¯•é€šè¿‡ç‰¹å®šçš„å›¾ç‰‡æ ‡ç­¾è·å–
            cover_img = soup.select_one(f'img[alt*="{self.movie_code}"]')
            if not cover_img:
                cover_img = soup.select_one(".aspect-video > img") or soup.select_one(
                    ".cover-image"
                )

            if cover_img and "src" in cover_img.attrs:
                result["cover_url"] = cover_img["src"]
                logger.info(f"ä»imgæ ‡ç­¾è·å–åˆ°å°é¢: {result['cover_url']}")
            else:
                # 2. å°è¯•ä»metaæ ‡ç­¾è·å–
                og_image = soup.select_one('meta[property="og:image"]')
                if og_image:
                    result["cover_url"] = og_image.get("content")
                    logger.info(f"ä»og:imageè·å–åˆ°å°é¢: {result['cover_url']}")

            # è·å–æè¿°ä¿¡æ¯
            og_desc = soup.select_one('meta[property="og:description"]')
            if og_desc and len(og_desc.get("content", "")) > 10:
                result["description"] = og_desc.get("content")
                logger.info(f"è·å–åˆ°æè¿°ä¿¡æ¯ï¼Œé•¿åº¦: {len(result['description'])}")

            # è·å–è§†é¢‘æ—¶é•¿
            og_duration = soup.select_one('meta[property="og:video:duration"]')
            if og_duration and og_duration.get("content", "").isdigit():
                result["duration_seconds"] = int(og_duration.get("content"))
                logger.info(f"è·å–åˆ°è§†é¢‘æ—¶é•¿: {result['duration_seconds']}ç§’")

            # æå–M3U8æµåª’ä½“URL
            try:
                # æå–åŠ å¯†çš„M3U8ä¿¡æ¯
                m3u8_info = self.extract_m3u8_info(html)
                # è§£å¯†M3U8 URL
                m3u8_urls = self.deobfuscate_m3u8(
                    m3u8_info["encrypted_code"], m3u8_info["dictionary"]
                )
                if m3u8_urls:
                    result["m3u8_urls"] = m3u8_urls
                    logger.info(f"æˆåŠŸæå–åˆ°{len(m3u8_urls)}ä¸ªM3U8æµåª’ä½“URL")
            except Exception as e:
                logger.error(f"æå–M3U8æµåª’ä½“URLå¤±è´¥: {e}")

            # è·å–å‘å¸ƒæ—¥æœŸ
            og_release_date = soup.select_one('meta[property="og:video:release_date"]')
            if og_release_date and og_release_date.get("content"):
                result["release_date"] = og_release_date.get("content")
                logger.info(f"ä»metaæ ‡ç­¾è·å–åˆ°å‘å¸ƒæ—¥æœŸ: {result['release_date']}")

            # è§£ææ ‡ç­¾
            # 1. å°è¯•é€šè¿‡Alpine.jsæ ‡è®°çš„div
            tags_div = soup.select_one("div[x-show=\"currentTab === 'tags'\"]")
            if tags_div:
                tags = [tag.text.strip() for tag in tags_div.select("a.tag")]
                if tags:
                    result["tags"] = tags
                    logger.info(f"ä»x-showæ ‡ç­¾è·å–åˆ°{len(tags)}ä¸ªæ ‡ç­¾")

            # 2. å°è¯•ä½¿ç”¨field_patternsè·å–
            if not result["tags"]:
                tags = self._extract_info_by_label(
                    soup, self.field_patterns["genre"][0]
                )
                if tags:
                    result["tags"] = tags
                    logger.info(f"ä»field_patternsè·å–åˆ°{len(tags)}ä¸ªæ ‡ç­¾")

            # 3. å°è¯•ä»metaå…³é”®è¯è·å–
            if not result["tags"]:
                meta_keywords = soup.select_one('meta[name="keywords"]')
                if meta_keywords and meta_keywords.get("content"):
                    keywords = meta_keywords.get("content").split(",")
                    result["tags"] = [
                        kw.strip()
                        for kw in keywords
                        if kw.strip() and kw.strip() != "ç„¡æ–™AV"
                    ]
                    logger.info(f"ä»metaå…³é”®è¯è·å–åˆ°{len(result['tags'])}ä¸ªæ ‡ç­¾")

            # è§£æå¥³ä¼˜
            # 1. å°è¯•é€šè¿‡Alpine.jsæ ‡è®°çš„div
            actresses_div = soup.select_one(
                "div[x-show=\"currentTab === 'actresses'\"]"
            )
            if actresses_div:
                actresses = [
                    actress.text.strip()
                    for actress in actresses_div.select("a.actress")
                ]
                if actresses:
                    result["actresses"] = actresses
                    logger.info(f"ä»x-showæ ‡ç­¾è·å–åˆ°{len(actresses)}ä¸ªå¥³ä¼˜")

            # 2. å°è¯•ä½¿ç”¨field_patternsè·å–
            if not result["actresses"]:
                actresses = self._extract_info_by_label(
                    soup, self.field_patterns["actress"][0]
                )
                if actresses:
                    result["actresses"] = actresses
                    logger.info(f"ä»field_patternsè·å–åˆ°{len(actresses)}ä¸ªå¥³ä¼˜")

            # è§£æç£åŠ›é“¾æ¥
            magnets_div = soup.select_one("div[x-show=\"currentTab === 'magnets'\"]")
            if magnets_div:
                magnet_rows = magnets_div.select("tbody tr")
                magnets = []
                for row in magnet_rows:
                    magnet_link = row.select_one('a[href^="magnet:"]')
                    if magnet_link:
                        magnet_url = magnet_link.get("href", "")
                        magnet_title = magnet_link.text.strip()

                        # è·å–å¤§å°å’Œæ—¥æœŸ
                        size_cell = (
                            row.select("td")[1] if len(row.select("td")) > 1 else None
                        )
                        date_cell = (
                            row.select("td")[2] if len(row.select("td")) > 2 else None
                        )

                        magnet_info = {
                            "url": magnet_url,
                            "title": magnet_title,
                            "size": size_cell.text.strip() if size_cell else "",
                            "date": date_cell.text.strip() if date_cell else "",
                        }
                        magnets.append(magnet_info)

                result["magnets"] = magnets
                logger.info(f"è·å–åˆ°{len(magnets)}ä¸ªç£åŠ›é“¾æ¥")

            # è·å–å·¥ä½œå®¤ã€å‚å•†ã€ç³»åˆ—ç­‰ä¿¡æ¯
            studio = self._extract_single_field_by_label(
                soup, self.field_patterns["studio"][0]
            )
            if studio:
                result["studio"] = studio
                logger.info(f"è·å–åˆ°å·¥ä½œå®¤: {result['studio']}")

            label = self._extract_single_field_by_label(
                soup, self.field_patterns["label"][0]
            )
            if label:
                result["label"] = label
                logger.info(f"è·å–åˆ°å‚å•†: {result['label']}")

            series = self._extract_single_field_by_label(
                soup, self.field_patterns["series"][0]
            )
            if series:
                result["series"] = series
                logger.info(f"è·å–åˆ°ç³»åˆ—: {result['series']}")

            director = self._extract_single_field_by_label(
                soup, self.field_patterns["director"][0]
            )
            if director:
                result["director"] = director
                logger.info(f"è·å–åˆ°å¯¼æ¼”: {result['director']}")

        except Exception as e:
            logger.error(f"è§£æé¡µé¢æ—¶å‡ºé”™: {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        return result

def test_parse_movie_page():
    """æµ‹è¯•è§£æç”µå½±é¡µé¢åŠŸèƒ½"""
    print("=== æµ‹è¯•MovieDetailCrawlerçš„parse_movie_pageæ–¹æ³• ===")
    
    # HTMLæ–‡ä»¶è·¯å¾„
    html_file = project_root / "test_536VOLA_data" / "HZHB-004_ja.html"
    
    if not html_file.exists():
        print(f"âŒ HTMLæ–‡ä»¶ä¸å­˜åœ¨: {html_file}")
        return False
    
    print(f"ğŸ“ è¯»å–HTMLæ–‡ä»¶: {html_file}")
    
    try:
        # è¯»å–HTMLå†…å®¹
        with open(html_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        print(f"ğŸ“„ HTMLæ–‡ä»¶å¤§å°: {len(html_content)} å­—ç¬¦")
        
        # åˆ›å»ºè§£æå™¨å®ä¾‹
        movie_code = "HZHB-004"
        crawler = SimpleMovieDetailCrawler(movie_code)
        
        print(f"ğŸ¬ è§£æç”µå½±: {movie_code}")
        
        # è°ƒç”¨è§£ææ–¹æ³•
        result = crawler.parse_movie_page(html_content)
        
        if result:
            print("âœ… è§£ææˆåŠŸ!")
            print("\n=== è§£æç»“æœ ===")
            
            # æ ¼å¼åŒ–è¾“å‡ºç»“æœ
            for key, value in result.items():
                if isinstance(value, list):
                    print(f"{key}: {value if value else '[]'}")
                elif isinstance(value, str) and len(value) > 100:
                    print(f"{key}: {value[:100]}...")
                else:
                    print(f"{key}: {value}")
            
            # ä¿å­˜è§£æç»“æœåˆ°JSONæ–‡ä»¶
            output_file = project_root / "test_536VOLA_data" / f"{movie_code}_parsed_test.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ è§£æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            # æ£€æŸ¥å…³é”®å­—æ®µ
            print("\n=== å…³é”®å­—æ®µæ£€æŸ¥ ===")
            key_fields = ['title', 'cover_url', 'm3u8_urls', 'description', 'tags', 'actresses']
            for field in key_fields:
                value = result.get(field)
                status = "âœ…" if value else "âŒ"
                print(f"{status} {field}: {'æœ‰å€¼' if value else 'æ— å€¼'}")
            
            return True
        else:
            print("âŒ è§£æå¤±è´¥: è¿”å›ç»“æœä¸ºç©º")
            return False
            
    except Exception as e:
        print(f"âŒ è§£æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•MovieDetailCrawlerè§£æåŠŸèƒ½...\n")
    
    success = test_parse_movie_page()
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")
    if success:
        print("âœ… æµ‹è¯•é€šè¿‡: è§£æå™¨å·¥ä½œæ­£å¸¸")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥: è§£æå™¨å­˜åœ¨é—®é¢˜")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())