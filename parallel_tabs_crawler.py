#!/usr/bin/env python3
"""
å¹¶è¡Œæ ‡ç­¾é¡µçˆ¬è™«
åŒæ—¶æ‰“å¼€5ä¸ªæ ‡ç­¾é¡µå¹¶è¡Œå¤„ç†ç”µå½±
"""

import time
import random
import json
import asyncio
from pathlib import Path
from loguru import logger
from DrissionPage import ChromiumPage, ChromiumOptions
from concurrent.futures import ThreadPoolExecutor
import threading

class ParallelTabsCrawler:
    """å¹¶è¡Œæ ‡ç­¾é¡µçˆ¬è™«"""
    
    def __init__(self, max_tabs=5):
        self.max_tabs = max_tabs
        self.browser = None
        self.tabs = []
        self.results = []
        self.failed_movies = []
        self.lock = threading.Lock()
        
    def create_browser_with_tabs(self):
        """åˆ›å»ºæµè§ˆå™¨å¹¶æ‰“å¼€å¤šä¸ªæ ‡ç­¾é¡µ"""
        logger.info(f"ğŸš€ åˆ›å»ºæµè§ˆå™¨å¹¶å‡†å¤‡ {self.max_tabs} ä¸ªæ ‡ç­¾é¡µ")
        
        # åˆ›å»ºæµè§ˆå™¨
        options = ChromiumOptions()
        options.headless(False)  # æ˜¾ç¤ºæµè§ˆå™¨ä¾¿äºè§‚å¯Ÿ
        options.set_argument('--window-size=1920,1080')
        options.set_argument('--disable-blink-features=AutomationControlled')
        
        self.browser = ChromiumPage(addr_or_opts=options)
        
        # é¦–å…ˆè®¿é—®ä¸»é¡µå»ºç«‹ä¼šè¯
        logger.info("ğŸ“± å»ºç«‹ä¼šè¯...")
        self.browser.get("https://missav.ai/")
        time.sleep(2)
        
        # åˆ›å»ºå¤šä¸ªæ ‡ç­¾é¡µ
        self.tabs = [self.browser]  # ç¬¬ä¸€ä¸ªæ ‡ç­¾é¡µå°±æ˜¯ä¸»é¡µé¢
        
        for i in range(self.max_tabs - 1):
            try:
                new_tab = self.browser.new_tab()
                self.tabs.append(new_tab)
                logger.info(f"âœ… åˆ›å»ºæ ‡ç­¾é¡µ {i+2}/{self.max_tabs}")
                time.sleep(0.5)  # çŸ­æš‚å»¶è¿Ÿé¿å…åˆ›å»ºè¿‡å¿«
            except Exception as e:
                logger.warning(f"åˆ›å»ºæ ‡ç­¾é¡µ {i+2} å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¯ æˆåŠŸåˆ›å»º {len(self.tabs)} ä¸ªæ ‡ç­¾é¡µ")
        return len(self.tabs)
    
    def extract_movie_info_from_tab(self, tab, movie_code):
        """ä»æ ‡ç­¾é¡µæå–ç”µå½±ä¿¡æ¯"""
        try:
            html = tab.html
            if not html or len(html) < 1000:
                return None
            
            html_lower = html.lower()
            
            # æ£€æŸ¥å†…å®¹è´¨é‡
            movie_indicators = 0
            if 'missav' in html_lower:
                movie_indicators += 1
            if movie_code.lower() in html_lower:
                movie_indicators += 1
            if any(word in html_lower for word in ['video', 'movie', 'player']):
                movie_indicators += 1
            
            if movie_indicators < 2:
                return None
            
            # æå–ä¿¡æ¯
            info = {
                'code': movie_code,
                'url': tab.url,
                'timestamp': time.time(),
                'page_length': len(html)
            }
            
            # æå–æ ‡é¢˜
            try:
                title_element = tab.ele('tag:h1')
                if title_element:
                    info['title'] = title_element.text.strip()
                else:
                    info['title'] = "æœªçŸ¥æ ‡é¢˜"
            except:
                info['title'] = "æœªçŸ¥æ ‡é¢˜"
            
            # æ£€æŸ¥è§†é¢‘å†…å®¹
            info['has_video_content'] = 'm3u8' in html_lower
            info['magnet_count'] = html_lower.count('magnet:')
            
            return info
            
        except Exception as e:
            logger.error(f"ä»æ ‡ç­¾é¡µæå–ä¿¡æ¯å‡ºé”™: {e}")
            return None
    
    def crawl_movie_in_tab(self, tab_index, movie_code):
        """åœ¨æŒ‡å®šæ ‡ç­¾é¡µä¸­çˆ¬å–ç”µå½±"""
        tab = self.tabs[tab_index]
        url = f"https://missav.ai/ja/{movie_code}"
        
        try:
            logger.info(f"ğŸ“ [æ ‡ç­¾é¡µ{tab_index+1}] è®¿é—®: {movie_code}")
            
            # è®¿é—®é¡µé¢
            tab.get(url)
            
            # å¿«é€Ÿæ£€æŸ¥åŠ è½½çŠ¶æ€
            for check in range(3):  # æœ€å¤šæ£€æŸ¥3ç§’
                time.sleep(1)
                html_length = len(tab.html) if tab.html else 0
                
                if html_length > 50000:
                    logger.info(f"âœ… [æ ‡ç­¾é¡µ{tab_index+1}] {movie_code} é¡µé¢å·²åŠ è½½ ({html_length} å­—ç¬¦)")
                    break
            
            # å¿«é€Ÿæ»šåŠ¨
            try:
                tab.scroll(500)
                time.sleep(0.3)
                tab.scroll(0)
            except:
                pass
            
            # æå–ä¿¡æ¯
            movie_info = self.extract_movie_info_from_tab(tab, movie_code)
            
            # çº¿ç¨‹å®‰å…¨åœ°ä¿å­˜ç»“æœ
            with self.lock:
                if movie_info:
                    self.results.append(movie_info)
                    logger.info(f"âœ… [æ ‡ç­¾é¡µ{tab_index+1}] {movie_code}: {movie_info.get('title', 'æœªçŸ¥')[:30]}...")
                    return True
                else:
                    self.failed_movies.append(movie_code)
                    logger.error(f"âŒ [æ ‡ç­¾é¡µ{tab_index+1}] {movie_code}: ä¿¡æ¯æå–å¤±è´¥")
                    return False
                    
        except Exception as e:
            logger.error(f"âŒ [æ ‡ç­¾é¡µ{tab_index+1}] {movie_code}: {e}")
            with self.lock:
                self.failed_movies.append(movie_code)
            return False
    
    def parallel_crawl_batch(self, movie_codes):
        """å¹¶è¡Œçˆ¬å–ä¸€æ‰¹ç”µå½±"""
        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {len(movie_codes)} éƒ¨ç”µå½±")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=len(self.tabs)) as executor:
            futures = []
            
            for i, movie_code in enumerate(movie_codes):
                tab_index = i % len(self.tabs)  # å¾ªç¯ä½¿ç”¨æ ‡ç­¾é¡µ
                future = executor.submit(self.crawl_movie_in_tab, tab_index, movie_code)
                futures.append((future, movie_code, tab_index))
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future, movie_code, tab_index in futures:
                try:
                    success = future.result(timeout=30)  # 30ç§’è¶…æ—¶
                except Exception as e:
                    logger.error(f"âŒ [æ ‡ç­¾é¡µ{tab_index+1}] {movie_code} è¶…æ—¶æˆ–å‡ºé”™: {e}")
                    with self.lock:
                        self.failed_movies.append(movie_code)
    
    def batch_crawl(self, movie_codes, batch_size=None):
        """æ‰¹é‡å¹¶è¡Œçˆ¬å–"""
        if batch_size is None:
            batch_size = self.max_tabs
        
        logger.info(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰¹é‡çˆ¬å– {len(movie_codes)} éƒ¨ç”µå½±")
        logger.info(f"ğŸ“Š ä½¿ç”¨ {len(self.tabs)} ä¸ªæ ‡ç­¾é¡µï¼Œæ¯æ‰¹å¤„ç† {batch_size} éƒ¨")
        
        start_time = time.time()
        
        try:
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(movie_codes), batch_size):
                batch = movie_codes[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(movie_codes) + batch_size - 1) // batch_size
                
                logger.info(f"\nğŸ¬ æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch)} éƒ¨ç”µå½±")
                logger.info(f"ğŸ“‹ {', '.join(batch)}")
                
                # å¹¶è¡Œå¤„ç†è¿™ä¸€æ‰¹
                self.parallel_crawl_batch(batch)
                
                # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯
                if i + batch_size < len(movie_codes):
                    rest_time = random.uniform(2, 5)
                    logger.info(f"ğŸ˜´ æ‰¹æ¬¡é—´ä¼‘æ¯ {rest_time:.1f} ç§’...")
                    time.sleep(rest_time)
                
                # ä¿å­˜ä¸­é—´ç»“æœ
                self.save_results()
                
                # æ˜¾ç¤ºè¿›åº¦
                elapsed = time.time() - start_time
                processed = min(i + batch_size, len(movie_codes))
                avg_time = elapsed / processed
                remaining = (len(movie_codes) - processed) * avg_time
                
                logger.info(f"ğŸ“Š è¿›åº¦: {processed}/{len(movie_codes)}, "
                          f"å¹³å‡ {avg_time:.1f}ç§’/éƒ¨, é¢„è®¡å‰©ä½™ {remaining/60:.1f}åˆ†é’Ÿ")
        
        finally:
            # æœ€ç»ˆä¿å­˜
            self.save_results()
            
            # å…³é—­æµè§ˆå™¨
            if self.browser:
                try:
                    self.browser.quit()
                    logger.info("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
                except:
                    pass
        
        # è¾“å‡ºç»Ÿè®¡
        total_time = time.time() - start_time
        success_count = len(self.results)
        failed_count = len(self.failed_movies)
        
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ“Š å¹¶è¡Œçˆ¬å–å®Œæˆ")
        logger.info(f"æ€»æ•°: {len(movie_codes)}")
        logger.info(f"æˆåŠŸ: {success_count}")
        logger.info(f"å¤±è´¥: {failed_count}")
        logger.info(f"æˆåŠŸç‡: {success_count/len(movie_codes)*100:.1f}%")
        logger.info(f"æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
        logger.info(f"å¹³å‡é€Ÿåº¦: {total_time/len(movie_codes):.1f} ç§’/éƒ¨")
        logger.info(f"å¹¶è¡Œæ•ˆç‡: æ¯”å•çº¿ç¨‹å¿« ~{self.max_tabs:.1f}x")
        
        return self.results
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        if self.results:
            success_file = Path("parallel_crawl_results.json")
            with open(success_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ æˆåŠŸç»“æœå·²ä¿å­˜: {success_file} ({len(self.results)} éƒ¨)")
        
        if self.failed_movies:
            failed_file = Path("parallel_failed_movies.json")
            with open(failed_file, 'w', encoding='utf-8') as f:
                json.dump(self.failed_movies, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“ å¤±è´¥åˆ—è¡¨å·²ä¿å­˜: {failed_file} ({len(self.failed_movies)} éƒ¨)")

def main():
    """ä¸»å‡½æ•°"""
    
    # æµ‹è¯•ç”µå½±åˆ—è¡¨
    test_movies = [
        "ipzz-562", "sone-718", "ngod-266", "dass-659", "jur-320",
        "ure-122", "ipzz-563", "sone-719", "ngod-267", "dass-660",
        "jur-321", "ure-123", "ipzz-564", "sone-720", "ngod-268"
    ]
    
    logger.info("ğŸš€ å¹¶è¡Œæ ‡ç­¾é¡µçˆ¬è™«")
    logger.info(f"ğŸ“‹ å‡†å¤‡çˆ¬å– {len(test_movies)} éƒ¨ç”µå½±")
    logger.info("âš¡ ä½¿ç”¨ 5 ä¸ªæ ‡ç­¾é¡µå¹¶è¡Œå¤„ç†")
    logger.info(f"ğŸ• é¢„è®¡æ€»æ—¶é—´: ~{len(test_movies)*2/60:.1f} åˆ†é’Ÿ (æ¯”å•çº¿ç¨‹å¿«5å€)")
    
    # è¯¢é—®æ˜¯å¦å¼€å§‹
    start = input(f"\nğŸš€ å¼€å§‹å¹¶è¡Œçˆ¬å– {len(test_movies)} éƒ¨ç”µå½±? [y/n]: ").lower()
    if start != 'y':
        logger.info("ğŸ‘‹ ä¸‹æ¬¡è§ï¼")
        return
    
    # åˆ›å»ºçˆ¬è™«
    crawler = ParallelTabsCrawler(max_tabs=5)
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    actual_tabs = crawler.create_browser_with_tabs()
    if actual_tabs == 0:
        logger.error("âŒ æ— æ³•åˆ›å»ºæ ‡ç­¾é¡µï¼Œé€€å‡º")
        return
    
    # å¼€å§‹å¹¶è¡Œçˆ¬å–
    results = crawler.batch_crawl(test_movies)
    
    # æ˜¾ç¤ºç»“æœ
    if results:
        logger.info("\nğŸ¬ æˆåŠŸçˆ¬å–çš„ç”µå½±:")
        for result in results[:5]:
            logger.info(f"  âœ… {result['code']}: {result.get('title', 'æœªçŸ¥')[:40]}...")
        
        if len(results) > 5:
            logger.info(f"  ... è¿˜æœ‰ {len(results)-5} éƒ¨ç”µå½±")

if __name__ == "__main__":
    main()
