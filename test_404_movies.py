#!/usr/bin/env python3
"""
æµ‹è¯•è¢«è¯¯åˆ¤ä¸º404çš„ç”µå½±
æ£€æŸ¥ä¸ºä»€ä¹ˆè¿™äº›å®é™…å­˜åœ¨çš„ç”µå½±è¢«åˆ¤æ–­ä¸º404
"""

import json
import time
import random
from pathlib import Path
from loguru import logger
from DrissionPage import ChromiumPage, ChromiumOptions

def test_404_movies():
    """æµ‹è¯•è¢«è¯¯åˆ¤ä¸º404çš„ç”µå½±"""
    
    # ä½ æåˆ°çš„å®é™…å­˜åœ¨ä½†è¢«è¯¯åˆ¤ä¸º404çš„ç”µå½±
    test_movies = [
        "aczd-222",
        "sone-714", 
        "dass-645",
        "dass-599",
        "sone-717",
        "sone-711"
    ]
    
    logger.info(f"ğŸ§ª æµ‹è¯• {len(test_movies)} éƒ¨è¢«è¯¯åˆ¤ä¸º404çš„ç”µå½±")
    
    # åˆ›å»ºæµè§ˆå™¨
    options = ChromiumOptions()
    options.headless(False)
    options.set_argument('--window-size=1920,1080')
    
    browser = ChromiumPage(addr_or_opts=options)
    
    # å»ºç«‹ä¼šè¯
    browser.get("https://missav.ai/")
    time.sleep(2)
    
    results = []
    
    try:
        for i, movie_code in enumerate(test_movies, 1):
            logger.info(f"\nğŸ¬ æµ‹è¯• {i}/{len(test_movies)}: {movie_code}")
            
            url = f"https://missav.ai/ja/{movie_code}"
            logger.info(f"ğŸ“ URL: {url}")
            
            # è®¿é—®é¡µé¢
            browser.get(url)
            
            # ç­‰å¾…åŠ è½½
            for check in range(5):
                time.sleep(1)
                html = browser.html
                current_url = browser.url
                
                logger.info(f"â³ æ£€æŸ¥ {check+1}/5: HTMLé•¿åº¦={len(html) if html else 0}, URL={current_url}")
                
                if html and len(html) > 50000:
                    logger.info(f"âœ… é¡µé¢å·²å……åˆ†åŠ è½½")
                    break
                elif check == 4:
                    logger.warning(f"âš ï¸ é¡µé¢åŠ è½½å¯èƒ½ä¸å®Œæ•´")
            
            # è¯¦ç»†åˆ†æé¡µé¢å†…å®¹
            html = browser.html
            current_url = browser.url
            
            if html:
                html_lower = html.lower()
                
                # æ£€æŸ¥å„ç§æŒ‡æ ‡
                analysis = {
                    'movie_code': movie_code,
                    'original_url': url,
                    'final_url': current_url,
                    'html_length': len(html),
                    'redirected': current_url != url,
                }
                
                # æ£€æŸ¥404æŒ‡æ ‡
                has_404_text = any(indicator in html_lower for indicator in [
                    '404', 'not found', 'page not found', 'ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                    'ãŠæ¢ã—ã®ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ', 'does not exist', 'error 404'
                ])
                analysis['has_404_text'] = has_404_text
                
                # æ£€æŸ¥æ˜¯å¦é‡å®šå‘åˆ°ä¸»é¡µ
                is_homepage = 'missav.ai' in current_url and current_url.count('/') <= 3
                analysis['is_homepage'] = is_homepage
                
                # æ£€æŸ¥ç”µå½±ç›¸å…³å†…å®¹
                has_video_content = any(keyword in html_lower for keyword in [
                    'video', 'movie', 'player', 'download', 'm3u8', 'magnet'
                ])
                analysis['has_video_content'] = has_video_content
                
                # æ£€æŸ¥åŸºæœ¬ç½‘é¡µå†…å®¹
                has_basic_content = any(keyword in html_lower for keyword in [
                    'missav', 'title', 'content', 'body', 'main'
                ])
                analysis['has_basic_content'] = has_basic_content
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”µå½±ä»£ç 
                has_movie_code = movie_code.lower() in html_lower
                analysis['has_movie_code'] = has_movie_code
                
                # æ£€æŸ¥é¡µé¢æ ‡é¢˜
                try:
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(html, 'html.parser')
                    title_tag = soup.find('title')
                    page_title = title_tag.get_text().strip() if title_tag else "æ— æ ‡é¢˜"
                    analysis['page_title'] = page_title
                    
                    h1_tag = soup.find('h1')
                    h1_text = h1_tag.get_text().strip() if h1_tag else "æ— H1"
                    analysis['h1_text'] = h1_text
                except:
                    analysis['page_title'] = "è§£æå¤±è´¥"
                    analysis['h1_text'] = "è§£æå¤±è´¥"
                
                # åˆ¤æ–­æ˜¯å¦åº”è¯¥è¢«è®¤ä¸ºæ˜¯404
                should_be_404 = (has_404_text or is_homepage or 
                                len(html) < 1000 or not has_basic_content)
                analysis['should_be_404'] = should_be_404
                
                # è®°å½•ç»“æœ
                results.append(analysis)
                
                # è¾“å‡ºåˆ†æç»“æœ
                logger.info(f"ğŸ“Š {movie_code} åˆ†æç»“æœ:")
                logger.info(f"  HTMLé•¿åº¦: {analysis['html_length']}")
                logger.info(f"  é‡å®šå‘: {analysis['redirected']}")
                logger.info(f"  404æ–‡æœ¬: {analysis['has_404_text']}")
                logger.info(f"  ä¸»é¡µé‡å®šå‘: {analysis['is_homepage']}")
                logger.info(f"  è§†é¢‘å†…å®¹: {analysis['has_video_content']}")
                logger.info(f"  åŸºæœ¬å†…å®¹: {analysis['has_basic_content']}")
                logger.info(f"  åŒ…å«ä»£ç : {analysis['has_movie_code']}")
                logger.info(f"  é¡µé¢æ ‡é¢˜: {analysis['page_title'][:50]}...")
                logger.info(f"  H1æ–‡æœ¬: {analysis['h1_text'][:50]}...")
                logger.info(f"  åº”ä¸º404: {analysis['should_be_404']}")
                
                if not should_be_404:
                    logger.info(f"âœ… {movie_code}: ç¡®å®å­˜åœ¨ï¼Œä¹‹å‰è¯¯åˆ¤ä¸º404")
                else:
                    logger.info(f"ğŸš« {movie_code}: ç¡®å®æ˜¯404æˆ–æ— æ•ˆé¡µé¢")
            else:
                logger.error(f"âŒ {movie_code}: æ— æ³•è·å–é¡µé¢å†…å®¹")
                results.append({
                    'movie_code': movie_code,
                    'original_url': url,
                    'final_url': current_url,
                    'error': 'no_html_content'
                })
            
            # é—´éš”
            if i < len(test_movies):
                time.sleep(random.uniform(3, 6))
    
    finally:
        browser.quit()
        logger.info("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = Path("404_analysis_results.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # ç»Ÿè®¡ç»“æœ
    valid_movies = [r for r in results if not r.get('should_be_404', True)]
    invalid_movies = [r for r in results if r.get('should_be_404', True)]
    
    logger.info(f"\n{'='*50}")
    logger.info("ğŸ“Š 404è¯¯åˆ¤åˆ†æç»“æœ")
    logger.info(f"æ€»æ•°: {len(test_movies)}")
    logger.info(f"ç¡®å®å­˜åœ¨ï¼ˆè¢«è¯¯åˆ¤ï¼‰: {len(valid_movies)}")
    logger.info(f"ç¡®å®æ˜¯404: {len(invalid_movies)}")
    
    if valid_movies:
        logger.info("\nâœ… ç¡®å®å­˜åœ¨çš„ç”µå½±:")
        for movie in valid_movies:
            logger.info(f"  - {movie['movie_code']}: {movie.get('page_title', 'æœªçŸ¥')[:50]}...")
    
    if invalid_movies:
        logger.info("\nğŸš« ç¡®å®æ˜¯404çš„ç”µå½±:")
        for movie in invalid_movies:
            logger.info(f"  - {movie['movie_code']}: {movie.get('error', '404é¡µé¢')}")
    
    return results

if __name__ == "__main__":
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•è¢«è¯¯åˆ¤ä¸º404çš„ç”µå½±")
    logger.info("ğŸ¯ æ£€æŸ¥ä¸ºä»€ä¹ˆå®é™…å­˜åœ¨çš„ç”µå½±è¢«åˆ¤æ–­ä¸º404")
    
    results = test_404_movies()
    
    logger.info("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    logger.info("ğŸ’¡ æ ¹æ®ç»“æœå¯ä»¥è°ƒæ•´404æ£€æµ‹é€»è¾‘")
