#!/usr/bin/env python3
"""
LinuxæœåŠ¡å™¨ç‰ˆç”µå½±çˆ¬è™«
é€‚é…æ— å¤´æ¨¡å¼å’ŒæœåŠ¡å™¨ç¯å¢ƒ
"""

import os
import sys
import time
import json
import random
import argparse
from pathlib import Path
from loguru import logger
from DrissionPage import ChromiumPage, ChromiumOptions
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    "src/logs/crawler.log",
    rotation="10 MB",
    retention=5,
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{line} - {message}",
    enqueue=True,
    encoding="utf-8"
)
logger.add(
    sys.stderr,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)

class LinuxMovieCrawler:
    """LinuxæœåŠ¡å™¨ç‰ˆç”µå½±çˆ¬è™«"""
    
    def __init__(self, headless=True, max_workers=3):
        self.headless = headless
        self.max_workers = max_workers
        self.output_file = "crawl_results.jsonl"
        
        # æ•°æ®åº“é…ç½®
        self.db_url = self.get_db_url()
        self.engine = create_engine(self.db_url)
        self.Session = sessionmaker(bind=self.engine)
        
        # æµè§ˆå™¨é…ç½®
        self.browser = None
        self.tabs = []
        
        logger.info("ğŸš€ Linuxç”µå½±çˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
    
    def get_db_url(self):
        """è·å–æ•°æ®åº“è¿æ¥URL"""
        # ä»ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶è¯»å–
        db_host = os.getenv('DB_HOST', 'localhost')
        db_port = os.getenv('DB_PORT', '5432')
        db_name = os.getenv('DB_NAME', 'movie_crawler')
        db_user = os.getenv('DB_USER', 'crawler_user')
        db_password = os.getenv('DB_PASSWORD', 'your_password')
        
        return f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"
    
    def setup_browser(self):
        """è®¾ç½®æµè§ˆå™¨"""
        options = ChromiumOptions()
        
        if self.headless:
            options.headless(True)
        
        # LinuxæœåŠ¡å™¨ä¼˜åŒ–é…ç½®
        options.set_argument('--no-sandbox')
        options.set_argument('--disable-dev-shm-usage')
        options.set_argument('--disable-gpu')
        options.set_argument('--disable-web-security')
        options.set_argument('--disable-features=VizDisplayCompositor')
        options.set_argument('--window-size=1920,1080')
        options.set_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        try:
            self.browser = ChromiumPage(addr_or_opts=options)
            logger.info("âœ… æµè§ˆå™¨åˆ›å»ºæˆåŠŸ")
            
            # å»ºç«‹ä¼šè¯
            self.browser.get("https://missav.ai/")
            time.sleep(3)
            
            # åˆ›å»ºå¤šä¸ªæ ‡ç­¾é¡µ
            self.tabs = [self.browser]
            for i in range(self.max_workers - 1):
                try:
                    new_tab = self.browser.new_tab()
                    self.tabs.append(new_tab)
                    logger.info(f"âœ… åˆ›å»ºæ ‡ç­¾é¡µ {i+2}/{self.max_workers}")
                    time.sleep(1)
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ›å»ºæ ‡ç­¾é¡µå¤±è´¥: {e}")
                    break
            
            logger.info(f"ğŸ¯ æˆåŠŸåˆ›å»º {len(self.tabs)} ä¸ªæ ‡ç­¾é¡µ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµè§ˆå™¨åˆ›å»ºå¤±è´¥: {e}")
            return False
    
    def get_movies_from_db(self, limit=None, offset_id=None):
        """ä»æ•°æ®åº“è·å–ç”µå½±åˆ—è¡¨"""
        session = self.Session()
        try:
            # æ„å»ºæŸ¥è¯¢
            query = """
                SELECT id, link 
                FROM movies 
                WHERE link IS NOT NULL 
                AND link != ''
            """
            
            if offset_id:
                query += f" AND id > {offset_id}"
            
            query += " ORDER BY id"
            
            if limit:
                query += f" LIMIT {limit}"
            
            result = session.execute(text(query))
            raw_movies = [(row.id, row.link) for row in result]
            
            # è½¬æ¢ä¸ºå®Œæ•´URLå¹¶ä¿®æ­£uncensored-leaked
            movies = []
            for movie_id, link in raw_movies:
                if link.startswith('dm3/v/') or link.startswith('dm4/v/'):
                    movie_code = link.split('/')[-1]
                    # ä¿®æ­£uncensored-leakedä¸ºuncensored-leak
                    if movie_code.endswith('-uncensored-leaked'):
                        original_code = movie_code
                        movie_code = movie_code.replace('-uncensored-leaked', '-uncensored-leak')
                        logger.info(f"ğŸ”§ ä¿®æ­£URL: ID={movie_id}, {original_code} â†’ {movie_code}")
                    full_url = f"https://missav.ai/ja/{movie_code}"
                    movies.append((movie_id, full_url, movie_code))
            
            logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å–åˆ° {len(movies)} éƒ¨ç”µå½±")
            return movies
            
        finally:
            session.close()
    
    def extract_movie_info(self, html, movie_id, movie_code, url):
        """æå–ç”µå½±ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            import re
            from bs4 import BeautifulSoup
            
            result = {
                'id': movie_id,
                'code': movie_code,
                'url': url,
                'timestamp': time.time(),
                'page_length': len(html),
                'extraction_type': 'linux_simple'
            }
            
            # æå–æ ‡é¢˜
            try:
                soup = BeautifulSoup(html, 'html.parser')
                h1_tag = soup.find('h1')
                if h1_tag:
                    result['title'] = h1_tag.get_text().strip()
                else:
                    result['title'] = movie_code
            except:
                result['title'] = movie_code
            
            # æå–M3U8é“¾æ¥
            m3u8_patterns = [
                r'https?://[^"\'>\s]+\.m3u8[^"\'>\s]*',
                r'"(https?://[^"]+\.m3u8[^"]*)"',
                r"'(https?://[^']+\.m3u8[^']*)'",
            ]
            
            all_m3u8 = []
            for pattern in m3u8_patterns:
                matches = re.findall(pattern, html)
                all_m3u8.extend(matches)
            
            unique_m3u8 = list(set(all_m3u8))[:5]
            result['m3u8_links'] = unique_m3u8
            
            # æå–ç£åŠ›é“¾æ¥
            magnet_links = re.findall(r'magnet:\?[^"\'>\s]+', html)
            result['magnet_links'] = magnet_links[:10]
            
            # è®¾ç½®çŠ¶æ€
            if len(unique_m3u8) > 0:
                result['status'] = 'success_with_m3u8'
                logger.info(f"âœ… æˆåŠŸæå–: {movie_code}, M3U8: {len(unique_m3u8)}, ç£åŠ›: {len(magnet_links)}")
            elif len(magnet_links) > 0:
                result['status'] = 'partial_success_magnet_only'
                logger.info(f"âš ï¸ éƒ¨åˆ†æˆåŠŸ: {movie_code}, ç£åŠ›: {len(magnet_links)}")
            else:
                result['status'] = 'extraction_failed'
                logger.warning(f"ğŸš« æå–å¤±è´¥: {movie_code}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ æå–å‡ºé”™: {movie_code}, é”™è¯¯: {e}")
            return None
    
    def crawl_movie(self, tab, movie_id, movie_url, movie_code):
        """çˆ¬å–å•éƒ¨ç”µå½±"""
        try:
            logger.info(f"ğŸ“ å¼€å§‹çˆ¬å–: ID={movie_id}, {movie_code}")
            
            # è®¿é—®é¡µé¢
            tab.get(movie_url)
            
            # ç­‰å¾…åŠ è½½
            for check in range(5):
                time.sleep(1)
                html = tab.html
                current_url = tab.url
                
                if html and len(html) > 50000:
                    logger.info(f"âœ… é¡µé¢å·²åŠ è½½: ID={movie_id} ({len(html)} å­—ç¬¦)")
                    break
                elif check == 4:
                    logger.warning(f"â³ é¡µé¢åŠ è½½è¶…æ—¶: ID={movie_id}")
            
            # æ£€æŸ¥é‡å®šå‘
            final_movie_code = movie_code
            if current_url != movie_url:
                try:
                    final_movie_code = current_url.split('/')[-1]
                    logger.info(f"ğŸ”„ é‡å®šå‘: {movie_code} â†’ {final_movie_code}")
                except:
                    pass
            
            # æå–ä¿¡æ¯
            if html and len(html) > 10000:
                movie_info = self.extract_movie_info(html, movie_id, final_movie_code, current_url)
                
                if movie_info:
                    # ä¿å­˜åˆ°JSONL
                    with open(self.output_file, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(movie_info, ensure_ascii=False) + '\n')
                    
                    return movie_info['status'], movie_info.get('title', movie_code)[:50]
                else:
                    return 'extraction_failed', f"{movie_code}: ä¿¡æ¯æå–å¤±è´¥"
            else:
                return '404_or_empty', f"{movie_code}: é¡µé¢å†…å®¹ä¸è¶³"
                
        except Exception as e:
            logger.error(f"âŒ çˆ¬å–å¼‚å¸¸: ID={movie_id}, é”™è¯¯: {e}")
            return 'exception', f"{movie_code}: {str(e)}"
    
    def run_batch(self, movies):
        """æ‰¹é‡å¤„ç†ç”µå½±"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        results = {
            'success': 0,
            'partial_success': 0,
            'failed': 0,
            'total': len(movies)
        }
        
        with ThreadPoolExecutor(max_workers=len(self.tabs)) as executor:
            # æäº¤ä»»åŠ¡
            futures = []
            for i, (movie_id, movie_url, movie_code) in enumerate(movies):
                tab = self.tabs[i % len(self.tabs)]
                future = executor.submit(self.crawl_movie, tab, movie_id, movie_url, movie_code)
                futures.append((future, movie_id, movie_code))
            
            # æ”¶é›†ç»“æœ
            for future, movie_id, movie_code in futures:
                try:
                    status, message = future.result(timeout=60)
                    
                    if status in ['success_with_m3u8']:
                        results['success'] += 1
                        logger.info(f"âœ… ID={movie_id}: {message}")
                    elif status in ['partial_success_magnet_only']:
                        results['partial_success'] += 1
                        logger.info(f"âš ï¸ ID={movie_id}: {message}")
                    else:
                        results['failed'] += 1
                        logger.warning(f"ğŸš« ID={movie_id}: {message}")
                        
                except Exception as e:
                    results['failed'] += 1
                    logger.error(f"ğŸ’¥ ID={movie_id}: å¤„ç†å¼‚å¸¸: {e}")
        
        return results
    
    def run(self, batch_size=10, max_movies=None):
        """è¿è¡Œçˆ¬è™«"""
        if not self.setup_browser():
            return False
        
        try:
            # è·å–æœ€åå¤„ç†çš„ID
            last_id = self.get_last_processed_id()
            logger.info(f"ğŸ“ ä»ID {last_id} å¼€å§‹å¤„ç†")
            
            total_results = {
                'success': 0,
                'partial_success': 0,
                'failed': 0,
                'total': 0
            }
            
            processed = 0
            while True:
                # è·å–ä¸€æ‰¹ç”µå½±
                movies = self.get_movies_from_db(limit=batch_size, offset_id=last_id)
                
                if not movies:
                    logger.info("ğŸ“Š æ²¡æœ‰æ›´å¤šç”µå½±éœ€è¦å¤„ç†")
                    break
                
                # å¤„ç†è¿™æ‰¹ç”µå½±
                logger.info(f"ğŸ¬ å¤„ç†æ‰¹æ¬¡: {len(movies)} éƒ¨ç”µå½±")
                batch_results = self.run_batch(movies)
                
                # ç´¯è®¡ç»“æœ
                for key in total_results:
                    total_results[key] += batch_results[key]
                
                processed += len(movies)
                last_id = movies[-1][0]  # æ›´æ–°æœ€åå¤„ç†çš„ID
                
                logger.info(f"ğŸ“Š æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ={batch_results['success']}, éƒ¨åˆ†æˆåŠŸ={batch_results['partial_success']}, å¤±è´¥={batch_results['failed']}")
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡
                if max_movies and processed >= max_movies:
                    logger.info(f"ğŸ“Š è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡: {max_movies}")
                    break
                
                # æ‰¹æ¬¡é—´éš”
                time.sleep(random.uniform(5, 10))
            
            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            logger.info("=" * 50)
            logger.info("ğŸ“Š çˆ¬å–å®Œæˆ")
            logger.info(f"æ€»æ•°: {total_results['total']}")
            logger.info(f"âœ… æˆåŠŸ: {total_results['success']}")
            logger.info(f"âš ï¸ éƒ¨åˆ†æˆåŠŸ: {total_results['partial_success']}")
            logger.info(f"ğŸš« å¤±è´¥: {total_results['failed']}")
            
            if total_results['total'] > 0:
                success_rate = (total_results['success'] + total_results['partial_success']) / total_results['total'] * 100
                logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
            
            return True
            
        finally:
            if self.browser:
                self.browser.quit()
                logger.info("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
    
    def get_last_processed_id(self):
        """è·å–æœ€åå¤„ç†çš„ID"""
        try:
            if os.path.exists(self.output_file):
                with open(self.output_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    if lines:
                        last_line = lines[-1].strip()
                        if last_line:
                            data = json.loads(last_line)
                            return data.get('id', 0)
            return 0
        except:
            return 0

def main():
    parser = argparse.ArgumentParser(description='Linuxç”µå½±çˆ¬è™«')
    parser.add_argument('--headless', action='store_true', default=True, help='æ— å¤´æ¨¡å¼')
    parser.add_argument('--workers', type=int, default=3, help='å¹¶å‘æ•°')
    parser.add_argument('--batch-size', type=int, default=10, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max-movies', type=int, help='æœ€å¤§å¤„ç†æ•°é‡')
    parser.add_argument('--daemon', action='store_true', help='å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼')
    
    args = parser.parse_args()
    
    # åŠ è½½ç¯å¢ƒå˜é‡
    env_file = Path('.env')
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value
    
    crawler = LinuxMovieCrawler(
        headless=args.headless,
        max_workers=args.workers
    )
    
    if args.daemon:
        # å®ˆæŠ¤è¿›ç¨‹æ¨¡å¼ - æŒç»­è¿è¡Œ
        while True:
            try:
                logger.info("ğŸ”„ å¼€å§‹æ–°ä¸€è½®çˆ¬å–...")
                crawler.run(batch_size=args.batch_size, max_movies=args.max_movies)
                logger.info("ğŸ˜´ ç­‰å¾…ä¸‹ä¸€è½®...")
                time.sleep(3600)  # ç­‰å¾…1å°æ—¶
            except KeyboardInterrupt:
                logger.info("ğŸ‘‹ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œé€€å‡º...")
                break
            except Exception as e:
                logger.error(f"ğŸ’¥ è¿è¡Œå¼‚å¸¸: {e}")
                time.sleep(300)  # ç­‰å¾…5åˆ†é’Ÿåé‡è¯•
    else:
        # å•æ¬¡è¿è¡Œæ¨¡å¼
        crawler.run(batch_size=args.batch_size, max_movies=args.max_movies)

if __name__ == "__main__":
    main()
