#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆæ•°æ®åº“çˆ¬è™« - 5ä¸ªå¹¶è¡Œï¼Œä½¿ç”¨parse_movie_pageï¼Œæ”¯æŒé‡è¯•
"""

import json
import time
import random
import re
import sys
from pathlib import Path
from loguru import logger
from DrissionPage import ChromiumPage, ChromiumOptions

# é…ç½®æ—¥å¿— - ç®€åŒ–ç‰ˆæœ¬ï¼Œä¿ç•™æœ€è¿‘çš„æ—¥å¿—
logger.remove()  # ç§»é™¤é»˜è®¤é…ç½®
logger.add(
    "src/logs/simple_crawler.log",
    rotation="5 MB",  # æ–‡ä»¶å¤§å°è½®è½¬ - å¤§çº¦5000è¡Œå·¦å³
    retention=3,  # ä¿ç•™3ä¸ªå¤‡ä»½æ–‡ä»¶
    level="INFO",
    format="{time:YYYY-MM-DD HH:mm:ss.SSS} | {level: <8} | {name}:{line} - {message}",
    enqueue=True,  # å¼‚æ­¥å†™å…¥
    encoding="utf-8"
)
logger.add(
    sys.stderr,
    level="INFO",
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>"
)
from concurrent.futures import ThreadPoolExecutor
import threading
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# å¯¼å…¥MovieDetailCrawlerå’Œæ—¥å¿—é…ç½®
sys.path.append(str(Path(__file__).parent / "src"))

# å¯¼å…¥MovieDetailCrawler
movie_crawler_error = None
try:
    from test.test_drission_movie import MovieDetailCrawler
    HAS_MOVIE_CRAWLER = True
except ImportError as e:
    HAS_MOVIE_CRAWLER = False
    movie_crawler_error = str(e)

# ç®€å•æ—¥å¿—é…ç½® - å·²åœ¨ä¸Šé¢é…ç½®å®Œæˆ

# è®°å½•MovieDetailCrawlerå¯¼å…¥çŠ¶æ€
if HAS_MOVIE_CRAWLER:
    logger.info("âœ… æˆåŠŸå¯¼å…¥MovieDetailCrawler")
else:
    logger.warning(f"âŒ æ— æ³•å¯¼å…¥MovieDetailCrawler: {movie_crawler_error}")

class SimpleDatabaseCrawler:
    """ç®€åŒ–ç‰ˆæ•°æ®åº“çˆ¬è™«"""
    
    def __init__(self):
        self.browser = None
        self.tabs = []
        self.results = []
        self.failed_movies = []
        self.lock = threading.Lock()
        self.max_retries = 3
        
        # æ•°æ®åº“è¿æ¥
        self.db_url = "postgresql://postgres:123456@localhost:5432/movie_crawler"
        self.engine = create_engine(self.db_url)
        self.Session = sessionmaker(bind=self.engine)
        
        # è¾“å‡ºæ–‡ä»¶
        self.output_file = Path("simple_crawl_results.jsonl")
        
    def get_last_processed_id(self):
        """ä»JSONLæ–‡ä»¶è·å–æœ€åå¤„ç†çš„ID"""
        if not self.output_file.exists():
            return 0
        
        last_id = 0
        try:
            with open(self.output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        if 'id' in data:
                            last_id = max(last_id, data['id'])
            logger.info(f"ğŸ“ æ‰¾åˆ°æœ€åå¤„ç†çš„ID: {last_id}")
        except Exception as e:
            logger.warning(f"è¯»å–æ–­ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
        
        return last_id
    
    def get_movies_from_database(self, start_id=0, limit=None):
        """ä»æ•°æ®åº“è·å–ç”µå½±åˆ—è¡¨"""
        session = self.Session()
        try:
            query = """
                SELECT id, link 
                FROM movies 
                WHERE id > :start_id 
                AND link IS NOT NULL 
                AND link != ''
                ORDER BY id ASC
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            result = session.execute(text(query), {"start_id": start_id})
            raw_movies = [(row.id, row.link) for row in result]
            
            # è½¬æ¢ä¸ºå®Œæ•´URLå¹¶ä¿®æ­£uncensored-leaked -> uncensored-leak
            movies = []
            for movie_id, link in raw_movies:
                if link.startswith('dm3/v/'):
                    movie_code = link.split('/')[-1]
                    # ä¿®æ­£uncensored-leakedä¸ºuncensored-leak
                    if movie_code.endswith('-uncensored-leaked'):
                        movie_code = movie_code.replace('-uncensored-leaked', '-uncensored-leak')
                        logger.info(f"ğŸ”§ ä¿®æ­£URL: ID={movie_id}, {link.split('/')[-1]} â†’ {movie_code}")
                    full_url = f"https://missav.ai/ja/{movie_code}"
                    movies.append((movie_id, full_url, movie_code))
                elif link.startswith('https://missav.ai/'):
                    movie_code = link.split('/')[-1]
                    # ä¿®æ­£uncensored-leakedä¸ºuncensored-leak
                    if movie_code.endswith('-uncensored-leaked'):
                        original_code = movie_code
                        movie_code = movie_code.replace('-uncensored-leaked', '-uncensored-leak')
                        logger.info(f"ğŸ”§ ä¿®æ­£URL: ID={movie_id}, {original_code} â†’ {movie_code}")
                        full_url = f"https://missav.ai/ja/{movie_code}"
                    else:
                        full_url = link
                    movies.append((movie_id, full_url, movie_code))
                else:
                    parts = link.split('/')
                    if len(parts) > 0:
                        movie_code = parts[-1]
                        # ä¿®æ­£uncensored-leakedä¸ºuncensored-leak
                        if movie_code.endswith('-uncensored-leaked'):
                            original_code = movie_code
                            movie_code = movie_code.replace('-uncensored-leaked', '-uncensored-leak')
                            logger.info(f"ğŸ”§ ä¿®æ­£URL: ID={movie_id}, {original_code} â†’ {movie_code}")
                        full_url = f"https://missav.ai/ja/{movie_code}"
                        movies.append((movie_id, full_url, movie_code))
            
            logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å–åˆ° {len(movies)} éƒ¨ç”µå½± (ID > {start_id})")
            return movies
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []
        finally:
            session.close()
    
    def create_browser_with_tabs(self):
        """åˆ›å»ºæµè§ˆå™¨å¹¶æ‰“å¼€5ä¸ªæ ‡ç­¾é¡µ"""
        logger.info("ğŸš€ åˆ›å»ºæµè§ˆå™¨å¹¶å‡†å¤‡5ä¸ªæ ‡ç­¾é¡µ")
        
        options = ChromiumOptions()
        options.headless(False)
        options.set_argument('--window-size=1920,1080')
        options.set_argument('--disable-blink-features=AutomationControlled')
        
        self.browser = ChromiumPage(addr_or_opts=options)


        # å»ºç«‹ä¼šè¯
        logger.info("ğŸ“± å»ºç«‹ä¼šè¯...")
        self.browser.get("https://missav.ai/")
        time.sleep(2)
        
        # åˆ›å»º5ä¸ªæ ‡ç­¾é¡µ
        self.tabs = [self.browser]
        
        for i in range(4):  # å†åˆ›å»º4ä¸ªï¼Œæ€»å…±5ä¸ª
            try:
                time.sleep(0.2)
                new_tab = self.browser.new_tab()
                self.tabs.append(new_tab)
                logger.info(f"âœ… åˆ›å»ºæ ‡ç­¾é¡µ {i+2}/5")
                time.sleep(0.5)
            except Exception as e:
                logger.warning(f"åˆ›å»ºæ ‡ç­¾é¡µ {i+2} å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¯ æˆåŠŸåˆ›å»º {len(self.tabs)} ä¸ªæ ‡ç­¾é¡µ")
        return len(self.tabs)
    
    def extract_with_parse_movie_page(self, html, movie_id, movie_code, url):
        """ä¼˜å…ˆæå–M3U8çš„ä¿¡æ¯æå–æ–¹æ³•"""
        try:
            if HAS_MOVIE_CRAWLER:
                # æ‰€æœ‰ç”µå½±éƒ½ä¼˜å…ˆæå–M3U8
                logger.info(f"ğŸ¯ ä¼˜å…ˆæå–M3U8: {movie_code}")

                # å¯¹äºuncensored-leakç”µå½±ï¼Œå¦‚æœparse_movie_pageå¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨æ­£åˆ™æå–
                if movie_code.endswith('-uncensored-leak'):
                    logger.info(f"ğŸ”§ uncensored-leakç”µå½±ï¼Œå°è¯•ç›´æ¥æ­£åˆ™æå–: {movie_code}")

                    # å…ˆå°è¯•parse_movie_page
                    try:
                        crawler = MovieDetailCrawler(movie_code)
                        result = crawler.parse_movie_page(html)

                        # æ£€æŸ¥æ˜¯å¦æˆåŠŸæå–åˆ°M3U8
                        m3u8_urls = result.get('m3u8_urls', [])
                        if len(m3u8_urls) > 0:
                            # æˆåŠŸæå–ï¼Œä½¿ç”¨å®Œæ•´ç»“æœ
                            result['id'] = movie_id
                            result['timestamp'] = time.time()
                            result['page_length'] = len(html)
                            result['m3u8_links'] = m3u8_urls
                            result['extraction_status'] = 'success_with_m3u8'
                            result['extraction_type'] = 'full_parse_m3u8'
                            logger.info(f"âœ… uncensored-leakå®Œæ•´æå–æˆåŠŸ: {movie_code}, M3U8æ•°é‡: {len(m3u8_urls)}")
                            return result
                    except Exception as e:
                        logger.warning(f"âš ï¸ uncensored-leakå®Œæ•´æå–å¤±è´¥: {movie_code}, é”™è¯¯: {e}")


                else:
                    # æ™®é€šç”µå½±ä½¿ç”¨å®Œæ•´çš„parse_movie_pageæ–¹æ³•
                    crawler = MovieDetailCrawler(movie_code)
                    result = crawler.parse_movie_page(html)

                    # æ·»åŠ æ•°æ®åº“ç›¸å…³å­—æ®µ
                    result['id'] = movie_id
                    result['timestamp'] = time.time()
                    result['page_length'] = len(html)

                    # æ£€æŸ¥M3U8æå–æƒ…å†µ - æ”¯æŒä¸¤ç§å­—æ®µå
                    m3u8_urls = result.get('m3u8_urls', [])  # MovieDetailCrawlerä½¿ç”¨çš„å­—æ®µå
                    m3u8_links = result.get('m3u8_links', [])  # å¤‡ç”¨å­—æ®µå

                    # ç»Ÿä¸€ä½¿ç”¨m3u8_linkså­—æ®µå
                    if m3u8_urls and not m3u8_links:
                        result['m3u8_links'] = m3u8_urls
                        m3u8_links = m3u8_urls

                    has_m3u8 = len(m3u8_links) > 0

                    if has_m3u8:
                        result['extraction_status'] = 'success_with_m3u8'
                        result['extraction_type'] = 'full_parse_m3u8'
                        logger.info(f"âœ… æˆåŠŸæå–M3U8: {movie_code}, M3U8æ•°é‡: {len(m3u8_links)}")
                        return result
                    else:
                        # å¦‚æœparse_movie_pageæ²¡æœ‰æå–åˆ°M3U8ï¼Œå°è¯•ç®€å•æ­£åˆ™æå–
                        logger.warning(f"âš ï¸ parse_movie_pageæœªæå–åˆ°M3U8ï¼Œå°è¯•æ­£åˆ™æå–: {movie_code}")

                        import re
                        # æå–M3U8é“¾æ¥
                        m3u8_links_regex = re.findall(r'https?://[^"\'>\s]+\.m3u8[^"\'>\s]*', html)

                        if m3u8_links_regex:
                            result['m3u8_links'] = m3u8_links_regex[:5]
                            result['extraction_status'] = 'success_with_regex_m3u8'
                            result['extraction_type'] = 'regex_m3u8'
                            logger.info(f"âœ… æ­£åˆ™æå–åˆ°M3U8: {movie_code}, M3U8æ•°é‡: {len(m3u8_links_regex)}")
                            return result
                        else:
                            # æ²¡æœ‰M3U8ä½†æœ‰å…¶ä»–ä¿¡æ¯ä¹Ÿç®—éƒ¨åˆ†æˆåŠŸ
                            result['extraction_status'] = 'partial_success_no_m3u8'
                            result['extraction_type'] = 'full_parse_no_m3u8'
                            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°M3U8ä½†æå–äº†å…¶ä»–ä¿¡æ¯: {movie_code}")
                            return result
            else:
                # ç®€åŒ–ç‰ˆæœ¬ - è‡³å°‘å°è¯•æå–M3U8
                import re
                m3u8_links = re.findall(r'https?://[^"\'>\s]+\.m3u8[^"\'>\s]*', html)
                magnet_links = re.findall(r'magnet:\?[^"\'>\s]+', html)

                info = {
                    'id': movie_id,
                    'code': movie_code,
                    'url': url,
                    'title': "ç®€åŒ–æå–",
                    'timestamp': time.time(),
                    'page_length': len(html),
                    'm3u8_links': m3u8_links[:5],
                    'magnet_links': magnet_links[:10],
                    'has_m3u8': len(m3u8_links) > 0,
                    'extraction_status': 'success_with_m3u8' if m3u8_links else 'fallback_no_m3u8',
                    'extraction_type': 'fallback'
                }
                logger.info(f"âœ… ç®€åŒ–æå–: {movie_code}, M3U8: {len(m3u8_links)}")
                return info

        except Exception as e:
            logger.error(f"æå–ä¿¡æ¯å‡ºé”™: {e}")
            return None

    def extract_uncensored_leak_with_regex(self, html, movie_id, movie_code, url):
        """ä¸“é—¨ä¸ºuncensored-leakç”µå½±è®¾è®¡çš„æ­£åˆ™æå–æ–¹æ³•"""
        try:
            import re
            from bs4 import BeautifulSoup

            logger.info(f"ğŸ”§ å¼€å§‹æ­£åˆ™æå–uncensored-leak: {movie_code}")

            # åŸºç¡€ä¿¡æ¯
            result = {
                'id': movie_id,
                'code': movie_code,
                'url': url,
                'timestamp': time.time(),
                'page_length': len(html),
                'extraction_type': 'regex_uncensored_leak'
            }

            # æå–æ ‡é¢˜
            try:
                soup = BeautifulSoup(html, 'html.parser')
                h1_tag = soup.find('h1')
                if h1_tag:
                    title = h1_tag.get_text().strip()
                    result['title'] = title
                    logger.info(f"âœ… æ­£åˆ™æå–æ ‡é¢˜: {title[:50]}...")
                else:
                    result['title'] = movie_code
            except:
                result['title'] = movie_code

            # æå–M3U8é“¾æ¥ - å¤šç§æ¨¡å¼
            m3u8_patterns = [
                r'https?://[^"\'>\s]+\.m3u8[^"\'>\s]*',
                r'"(https?://[^"]+\.m3u8[^"]*)"',
                r"'(https?://[^']+\.m3u8[^']*)'",
            ]

            all_m3u8 = []
            for pattern in m3u8_patterns:
                matches = re.findall(pattern, html)
                all_m3u8.extend(matches)

            # å»é‡å¹¶é™åˆ¶æ•°é‡
            unique_m3u8 = list(set(all_m3u8))[:5]
            result['m3u8_links'] = unique_m3u8
            result['m3u8_urls'] = unique_m3u8  # å…¼å®¹å­—æ®µ

            # æå–ç£åŠ›é“¾æ¥
            magnet_links = re.findall(r'magnet:\?[^"\'>\s]+', html)
            result['magnet_links'] = magnet_links[:10]

            # æå–å°é¢
            try:
                cover_match = re.search(r'og:image["\']?\s*content=["\']([^"\']+)', html)
                if cover_match:
                    result['cover'] = cover_match.group(1)
            except:
                pass

            # æå–æ—¶é•¿
            try:
                duration_match = re.search(r'(\d+):\d+:\d+', html)
                if duration_match:
                    hours = int(duration_match.group(1))
                    minutes_match = re.search(r'\d+:(\d+):\d+', html)
                    seconds_match = re.search(r'\d+:\d+:(\d+)', html)
                    if minutes_match and seconds_match:
                        minutes = int(minutes_match.group(1))
                        seconds = int(seconds_match.group(1))
                        total_seconds = hours * 3600 + minutes * 60 + seconds
                        result['duration'] = total_seconds
            except:
                pass

            # è®¾ç½®æå–çŠ¶æ€
            if len(unique_m3u8) > 0:
                result['extraction_status'] = 'success_with_regex_m3u8'
                logger.info(f"âœ… æ­£åˆ™æå–æˆåŠŸ: {movie_code}, M3U8: {len(unique_m3u8)}, ç£åŠ›: {len(magnet_links)}")
            elif len(magnet_links) > 0:
                result['extraction_status'] = 'partial_success_magnet_only'
                logger.info(f"âš ï¸ æ­£åˆ™æå–éƒ¨åˆ†æˆåŠŸ: {movie_code}, æ— M3U8ä½†æœ‰ç£åŠ›: {len(magnet_links)}")
            else:
                result['extraction_status'] = 'regex_extraction_failed'
                logger.warning(f"ğŸš« æ­£åˆ™æå–å¤±è´¥: {movie_code}, æ— M3U8å’Œç£åŠ›")

            return result

        except Exception as e:
            logger.error(f"âŒ æ­£åˆ™æå–å‡ºé”™: {movie_code}, é”™è¯¯: {e}")
            return {
                'id': movie_id,
                'code': movie_code,
                'url': url,
                'title': movie_code,
                'timestamp': time.time(),
                'page_length': len(html),
                'extraction_type': 'regex_failed',
                'extraction_status': 'regex_extraction_error',
                'error': str(e)
            }
    
    def check_404_or_not_found(self, html, current_url, original_url=None, movie_code=None):
        """æ£€æŸ¥é¡µé¢æ˜¯å¦ä¸º404æˆ–ä¸å­˜åœ¨ï¼ˆæ™ºèƒ½æ£€æµ‹ï¼‰"""
        if not html:
            return True

        html_lower = html.lower()

        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°ä¸»é¡µï¼ˆURLè·¯å¾„å¤ªçŸ­ï¼‰
        if 'missav.ai' in current_url and current_url.count('/') <= 3:
            logger.info(f"ğŸš« é‡å®šå‘åˆ°ä¸»é¡µ: {current_url}")
            return True

        # æ£€æŸ¥é¡µé¢å†…å®¹æ˜¯å¦è¿‡å°‘
        if len(html) < 1000:
            logger.info(f"ğŸš« é¡µé¢å†…å®¹è¿‡å°‘: {len(html)} å­—ç¬¦")
            return True

        # æ™ºèƒ½404æ£€æµ‹ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å®Œæ•´çš„ç”µå½±ä¿¡æ¯
        try:
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')

            # æ£€æŸ¥æ˜¯å¦æœ‰ç”µå½±æ ‡é¢˜ï¼ˆH1æ ‡ç­¾ï¼‰
            h1_tag = soup.find('h1')
            has_movie_title = h1_tag and len(h1_tag.get_text().strip()) > 10

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç”µå½±ä»£ç 
            has_movie_code = movie_code and movie_code.lower() in html_lower

            # æ£€æŸ¥æ˜¯å¦æœ‰è§†é¢‘ç›¸å…³å†…å®¹
            has_video_content = any(keyword in html_lower for keyword in [
                'video', 'player', 'm3u8', 'magnet', 'download'
            ])

            # å¦‚æœæœ‰å®Œæ•´çš„ç”µå½±ä¿¡æ¯ï¼Œå³ä½¿åŒ…å«"404"æ–‡æœ¬ä¹Ÿè®¤ä¸ºæ˜¯æœ‰æ•ˆé¡µé¢
            if has_movie_title and has_movie_code and has_video_content:
                logger.info(f"âœ… æ£€æµ‹åˆ°å®Œæ•´ç”µå½±ä¿¡æ¯ï¼Œè®¤ä¸ºæ˜¯æœ‰æ•ˆé¡µé¢")
                return False

            # åªæœ‰åœ¨ç¼ºå°‘å…³é”®ä¿¡æ¯æ—¶æ‰æ£€æŸ¥404æ–‡æœ¬
            has_404_text = any(indicator in html_lower for indicator in [
                'page not found', 'ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'ãŠæ¢ã—ã®ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ', 'does not exist', 'error 404'
            ])

            # æ›´ä¸¥æ ¼çš„404æ£€æµ‹ï¼šå¿…é¡»åŒæ—¶æ»¡è¶³å¤šä¸ªæ¡ä»¶
            if has_404_text and not (has_movie_title and has_movie_code):
                logger.info(f"ğŸš« æ£€æµ‹åˆ°404é”™è¯¯é¡µé¢ï¼ˆç¼ºå°‘ç”µå½±ä¿¡æ¯ï¼‰")
                return True

        except Exception as e:
            logger.warning(f"HTMLè§£æå‡ºé”™: {e}")

        # æœ€åçš„å…œåº•æ£€æŸ¥
        if not any(keyword in html_lower for keyword in [
            'missav', 'video', 'movie', 'title', 'content', 'body'
        ]):
            logger.warning(f"ğŸš« é¡µé¢ä¸åŒ…å«ä»»ä½•æœ‰æ•ˆå†…å®¹æ ‡è¯†")
            return True

        return False

    def create_404_placeholder(self, movie_id, movie_code, movie_url):
        """åˆ›å»º404å ä½ç¬¦"""
        return {
            'id': movie_id,
            'code': movie_code,
            'url': movie_url,
            'status': '404',
            'title': 'NOT_FOUND',
            'error': 'Movie not found or page does not exist',
            'timestamp': time.time(),
            'page_length': 0
        }

    def crawl_single_movie_with_retry(self, tab, movie_data):
        """çˆ¬å–å•ä¸ªç”µå½±ï¼ˆå¸¦é‡è¯•å’Œ404æ£€æµ‹ï¼Œå¤„ç†é‡å®šå‘ï¼‰"""
        movie_id, movie_url, movie_code = movie_data

        for attempt in range(self.max_retries):
            try:
                logger.info(f"ğŸ“ å°è¯• {attempt+1}/{self.max_retries}: ID={movie_id}, {movie_code}")

                # è®¿é—®é¡µé¢
                tab.get(movie_url)

                # ç®€å•ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼ˆæµè§ˆå™¨è‡ªåŠ¨å¤„ç†é‡å®šå‘ï¼‰
                for check in range(3):
                    time.sleep(0.3)
                    html = tab.html
                    current_url = tab.url

                    # æ£€æŸ¥é¡µé¢æ˜¯å¦åŠ è½½å®Œæˆ
                    if html and len(html) > 50000:
                        logger.info(f"âœ… ID={movie_id} é¡µé¢å·²åŠ è½½ ({len(html)} å­—ç¬¦)")
                        if current_url != movie_url:
                            logger.info(f"ğŸ”„ ID={movie_id} æœ€ç»ˆURL: {current_url}")
                        break
                    elif check == 4:  # æœ€åä¸€æ¬¡æ£€æŸ¥
                        logger.warning(f"â³ ID={movie_id} é¡µé¢åŠ è½½è¶…æ—¶ï¼Œå†…å®¹é•¿åº¦: {len(html) if html else 0}")

                # æ£€æŸ¥æ˜¯å¦ä¸º404æˆ–ä¸å­˜åœ¨
                html = tab.html
                current_url = tab.url

                if self.check_404_or_not_found(html, current_url, movie_url, movie_code):
                    logger.warning(f"ğŸš« ID={movie_id}: æ£€æµ‹åˆ°404æˆ–é¡µé¢ä¸å­˜åœ¨")
                    placeholder = self.create_404_placeholder(movie_id, movie_code, movie_url)
                    return placeholder

                # æå–ä¿¡æ¯
                if html and len(html) > 10000:
                    # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿäº†é‡å®šå‘ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨é‡å®šå‘åçš„ç”µå½±ä»£ç 
                    final_movie_code = movie_code
                    if current_url != movie_url:
                        # ä»æœ€ç»ˆURLæå–ç”µå½±ä»£ç 
                        try:
                            final_movie_code = current_url.split('/')[-1]
                            logger.info(f"ğŸ”„ ID={movie_id}: ä½¿ç”¨é‡å®šå‘åçš„ä»£ç : {movie_code} â†’ {final_movie_code}")
                        except:
                            logger.warning(f"âš ï¸ ID={movie_id}: æ— æ³•ä»é‡å®šå‘URLæå–ä»£ç ï¼Œä½¿ç”¨åŸå§‹ä»£ç ")

                    movie_info = self.extract_with_parse_movie_page(html, movie_id, final_movie_code, current_url)

                    if movie_info:
                        # ä¼˜å…ˆæ£€æŸ¥M3U8 - æœ‰M3U8å°±ç®—æˆåŠŸ
                        m3u8_links = movie_info.get('m3u8_links', [])
                        has_m3u8 = len(m3u8_links) > 0

                        if has_m3u8:
                            movie_info['status'] = 'success'
                            movie_info['success_reason'] = f'has_m3u8_{len(m3u8_links)}'
                            logger.info(f"âœ… ID={movie_id}: æˆåŠŸæå–M3U8 ({len(m3u8_links)}ä¸ª)")
                            return movie_info
                        elif movie_info.get('title') and movie_info.get('title') != "æœªçŸ¥æ ‡é¢˜":
                            # æ²¡æœ‰M3U8ä½†æœ‰æ ‡é¢˜ç­‰å…¶ä»–ä¿¡æ¯ä¹Ÿç®—æˆåŠŸ
                            movie_info['status'] = 'success'
                            movie_info['success_reason'] = 'has_title_no_m3u8'
                            logger.info(f"âœ… ID={movie_id}: æˆåŠŸæå–ä¿¡æ¯ï¼ˆæ— M3U8ï¼‰")
                            return movie_info
                        else:
                            logger.warning(f"âš ï¸ ID={movie_id}: ä¿¡æ¯æå–ä¸å®Œæ•´ï¼Œé‡è¯•")
                            if attempt < self.max_retries - 1:
                                time.sleep(0.5)
                                continue
                    else:
                        logger.warning(f"âš ï¸ ID={movie_id}: ä¿¡æ¯æå–å¤±è´¥ï¼Œé‡è¯•")
                        if attempt < self.max_retries - 1:
                            time.sleep(0.5)
                            continue
                else:
                    logger.warning(f"âš ï¸ ID={movie_id}: é¡µé¢å†…å®¹ä¸è¶³ï¼Œé‡è¯•")
                    if attempt < self.max_retries - 1:
                        time.sleep(0.5)
                        continue

            except Exception as e:
                logger.error(f"âŒ ID={movie_id} å°è¯• {attempt+1} å¤±è´¥: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2)
                    continue

        logger.error(f"ğŸ’€ ID={movie_id}: 3æ¬¡é‡è¯•å‡å¤±è´¥")
        # è¿”å›å¤±è´¥å ä½ç¬¦
        return {
            'id': movie_id,
            'code': movie_code,
            'url': movie_url,
            'status': 'failed',
            'title': 'EXTRACTION_FAILED',
            'error': 'Failed after 3 retries',
            'timestamp': time.time(),
            'page_length': 0
        }
    
    def save_result(self, movie_info):
        """ä¿å­˜å•ä¸ªç»“æœ"""
        try:
            with open(self.output_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(movie_info, ensure_ascii=False) + '\n')
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def crawl_batch(self, movies):
        """çˆ¬å–ä¸€æ‰¹ç”µå½±ï¼ˆ5ä¸ªå¹¶è¡Œï¼‰"""
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å– {len(movies)} éƒ¨ç”µå½±")
        
        # åˆ›å»ºæµè§ˆå™¨
        if self.create_browser_with_tabs() == 0:
            logger.error("âŒ æ— æ³•åˆ›å»ºæ ‡ç­¾é¡µ")
            return
        
        start_time = time.time()
        
        try:
            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹5ä¸ª
            for i in range(0, len(movies), 5):
                batch = movies[i:i + 5]
                batch_num = i // 5 + 1
                total_batches = (len(movies) + 4) // 5
                
                logger.info(f"\nğŸ¬ æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch)} éƒ¨ç”µå½±")
                
                # å¹¶è¡Œå¤„ç†è¿™ä¸€æ‰¹
                with ThreadPoolExecutor(max_workers=len(self.tabs)) as executor:
                    futures = []
                    
                    for j, movie_data in enumerate(batch):
                        tab = self.tabs[j % len(self.tabs)]
                        future = executor.submit(self.crawl_single_movie_with_retry, tab, movie_data)
                        futures.append((future, movie_data))
                    
                    # æ”¶é›†ç»“æœ
                    for future, movie_data in futures:
                        movie_id = movie_data[0]
                        try:
                            result = future.result(timeout=60)
                            if result:
                                with self.lock:
                                    self.results.append(result)
                                    self.save_result(result)

                                    # æ ¹æ®çŠ¶æ€æ˜¾ç¤ºä¸åŒä¿¡æ¯
                                    if result.get('status') == '404':
                                        logger.info(f"ğŸš« ID={result['id']}: 404 NOT_FOUND")
                                    elif result.get('status') == 'failed':
                                        logger.info(f"ğŸ’€ ID={result['id']}: EXTRACTION_FAILED")
                                    else:
                                        logger.info(f"âœ… ID={result['id']}: {result.get('title', 'æœªçŸ¥')[:30]}...")
                            else:
                                # è¿™ç§æƒ…å†µä¸åº”è¯¥å‘ç”Ÿï¼Œä½†ä»¥é˜²ä¸‡ä¸€
                                movie_id, movie_url, movie_code = movie_data
                                error_result = {
                                    'id': movie_id,
                                    'code': movie_code,
                                    'url': movie_url,
                                    'status': 'error',
                                    'title': 'UNKNOWN_ERROR',
                                    'error': 'Unexpected None result',
                                    'timestamp': time.time(),
                                    'page_length': 0
                                }
                                with self.lock:
                                    self.results.append(error_result)
                                    self.save_result(error_result)
                                    logger.error(f"â“ ID={movie_id}: UNKNOWN_ERROR")
                        except Exception as e:
                            logger.error(f"âŒ ID={movie_id} å¤„ç†å¼‚å¸¸: {e}")
                            # åˆ›å»ºå¼‚å¸¸å ä½ç¬¦
                            movie_id, movie_url, movie_code = movie_data
                            exception_result = {
                                'id': movie_id,
                                'code': movie_code,
                                'url': movie_url,
                                'status': 'exception',
                                'title': 'PROCESSING_EXCEPTION',
                                'error': str(e),
                                'timestamp': time.time(),
                                'page_length': 0
                            }
                            with self.lock:
                                self.results.append(exception_result)
                                self.save_result(exception_result)
                                logger.error(f"ğŸ’¥ ID={movie_id}: PROCESSING_EXCEPTION")
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if i + 5 < len(movies):
                    rest_time = random.uniform(2, 3)
                    logger.info(f"ğŸ˜´ æ‰¹æ¬¡é—´ä¼‘æ¯ {rest_time:.1f} ç§’...")
                    time.sleep(rest_time)
        
        finally:
            # å…³é—­æµè§ˆå™¨
            if self.browser:
                try:
                    self.browser.quit()
                    logger.info("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
                except:
                    pass
        
        # è¾“å‡ºç»Ÿè®¡
        total_time = time.time() - start_time

        # ç»Ÿè®¡ä¸åŒçŠ¶æ€
        success_count = len([r for r in self.results if r.get('status') == 'success'])
        not_found_count = len([r for r in self.results if r.get('status') == '404'])
        failed_count = len([r for r in self.results if r.get('status') == 'failed'])
        exception_count = len([r for r in self.results if r.get('status') in ['exception', 'error']])
        total_processed = len(self.results)

        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ“Š çˆ¬å–å®Œæˆ")
        logger.info(f"æ€»æ•°: {len(movies)}")
        logger.info(f"å·²å¤„ç†: {total_processed}")
        logger.info(f"âœ… æˆåŠŸ: {success_count}")
        logger.info(f"ğŸš« 404ä¸å­˜åœ¨: {not_found_count}")
        logger.info(f"ğŸ’€ æå–å¤±è´¥: {failed_count}")
        logger.info(f"ğŸ’¥ å¼‚å¸¸é”™è¯¯: {exception_count}")
        logger.info(f"æˆåŠŸç‡: {success_count/len(movies)*100:.1f}%")
        logger.info(f"æœ‰æ•ˆå¤„ç†ç‡: {total_processed/len(movies)*100:.1f}%")
        logger.info(f"æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {self.output_file}")

        # æ˜¾ç¤º404ç”µå½±çš„ä»£ç ï¼ˆä¾¿äºæ£€æŸ¥ï¼‰
        not_found_movies = [r['code'] for r in self.results if r.get('status') == '404']
        if not_found_movies:
            logger.info(f"ğŸš« 404ç”µå½±ä»£ç : {', '.join(not_found_movies[:10])}")
            if len(not_found_movies) > 10:
                logger.info(f"   ... è¿˜æœ‰ {len(not_found_movies)-10} ä¸ª")

def main():
    """ä¸»å‡½æ•°"""
    
    logger.info("ğŸš€ ç®€åŒ–ç‰ˆæ•°æ®åº“çˆ¬è™«")
    logger.info("ğŸ“Š åŠŸèƒ½ç‰¹æ€§:")
    logger.info("  - 5ä¸ªæ ‡ç­¾é¡µå¹¶è¡Œ")
    logger.info("  - ä½¿ç”¨parse_movie_pageæ–¹æ³•")
    logger.info("  - å¤±è´¥é‡è¯•3æ¬¡")
    logger.info("  - æ”¯æŒæ–­ç‚¹ç»§ç»­")
    
    # è¯¢é—®å¤„ç†æ•°é‡
    limit_input = input("\nğŸ”¢ é™åˆ¶å¤„ç†æ•°é‡ (å›è½¦=ä¸é™åˆ¶): ").strip()
    limit = int(limit_input) if limit_input.isdigit() else None
    
    # ç¡®è®¤å¼€å§‹
    start = input(f"\nğŸš€ å¼€å§‹çˆ¬å–? [y/n]: ").lower()
    if start != 'y':
        logger.info("ğŸ‘‹ ä¸‹æ¬¡è§ï¼")
        return
    
    # åˆ›å»ºçˆ¬è™«
    crawler = SimpleDatabaseCrawler()
    
    # è·å–æ–­ç‚¹
    start_id = crawler.get_last_processed_id()
    
    # è·å–ç”µå½±åˆ—è¡¨
    movies = crawler.get_movies_from_database(start_id, limit)
    if not movies:
        logger.info("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„ç”µå½±")
        return
    
    # å¼€å§‹çˆ¬å–
    crawler.crawl_batch(movies)

if __name__ == "__main__":
    main()
