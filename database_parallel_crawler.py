#!/usr/bin/env python3
"""
æ•°æ®åº“å¹¶è¡Œçˆ¬è™« - æ”¯æŒæ–­ç‚¹ç»§ç»­
ä»æ•°æ®åº“æŒ‰IDé¡ºåºå¤„ç†ï¼Œ10ä¸ªå¹¶è¡Œï¼Œæ”¯æŒæ–­ç‚¹ç»§ç»­
"""

import json
import time
import random
import re
import sys
from pathlib import Path
from loguru import logger
from DrissionPage import ChromiumPage, ChromiumOptions
from concurrent.futures import ThreadPoolExecutor
import threading
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
from bs4 import BeautifulSoup

# æ·»åŠ srcè·¯å¾„ä»¥å¯¼å…¥æµ‹è¯•æ¨¡å—
src_path = Path(__file__).parent / "src"
sys.path.append(str(src_path))
try:
    from test.test_drission_movie import MovieDetailCrawler
    HAS_DRISSION_CRAWLER = True
    logger.info("æˆåŠŸå¯¼å…¥MovieDetailCrawler")
except ImportError:
    HAS_DRISSION_CRAWLER = False
    logger.warning("æ— æ³•å¯¼å…¥MovieDetailCrawlerï¼Œå°†ä½¿ç”¨ç®€åŒ–çš„HTMLè§£æ")

class DatabaseParallelCrawler:
    """æ•°æ®åº“å¹¶è¡Œçˆ¬è™«"""
    
    def __init__(self, max_tabs=5, batch_size=5):
        self.max_tabs = max_tabs
        self.batch_size = batch_size
        self.browser = None
        self.tabs = []
        self.results = []
        self.failed_movies = []
        self.lock = threading.Lock()
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # æ•°æ®åº“è¿æ¥
        self.db_url = "postgresql://postgres:123456@localhost:5432/movie_crawler"
        self.engine = create_engine(self.db_url)
        self.Session = sessionmaker(bind=self.engine)
        
        # è¾“å‡ºæ–‡ä»¶
        self.output_file = Path("database_crawl_results.jsonl")
        
    def get_last_processed_id(self):
        """ä»JSONLæ–‡ä»¶è·å–æœ€åå¤„ç†çš„ID"""
        if not self.output_file.exists():
            return 0
        
        last_id = 0
        try:
            with open(self.output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line.strip())
                        if 'id' in data:
                            last_id = max(last_id, data['id'])
            logger.info(f"ğŸ“ æ‰¾åˆ°æœ€åå¤„ç†çš„ID: {last_id}")
        except Exception as e:
            logger.warning(f"è¯»å–æ–­ç‚¹æ–‡ä»¶å¤±è´¥: {e}")
        
        return last_id
    
    def get_movies_from_database(self, start_id=0, limit=None):
        """ä»æ•°æ®åº“è·å–ç”µå½±åˆ—è¡¨"""
        session = self.Session()
        try:
            query = """
                SELECT id, link
                FROM movies
                WHERE id > :start_id
                AND link IS NOT NULL
                AND link != ''
                ORDER BY id ASC
            """

            if limit:
                query += f" LIMIT {limit}"

            result = session.execute(text(query), {"start_id": start_id})
            raw_movies = [(row.id, row.link) for row in result]

            # è½¬æ¢ä¸ºå®Œæ•´URL
            movies = []
            for movie_id, link in raw_movies:
                # å¦‚æœlinkæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºå®Œæ•´URL
                if link.startswith('dm3/v/'):
                    # æå–ç”µå½±ä»£ç 
                    movie_code = link.split('/')[-1]  # ä¾‹å¦‚: dm3/v/345simm-656 -> 345simm-656
                    full_url = f"https://missav.ai/ja/{movie_code}"
                    movies.append((movie_id, full_url))
                elif link.startswith('https://missav.ai/'):
                    # å·²ç»æ˜¯å®Œæ•´URL
                    movies.append((movie_id, link))
                else:
                    # å…¶ä»–æ ¼å¼ï¼Œå°è¯•æå–ç”µå½±ä»£ç 
                    parts = link.split('/')
                    if len(parts) > 0:
                        movie_code = parts[-1]
                        full_url = f"https://missav.ai/ja/{movie_code}"
                        movies.append((movie_id, full_url))

            logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å–åˆ° {len(movies)} éƒ¨ç”µå½± (ID > {start_id})")
            if movies:
                logger.info(f"ğŸ“‹ ç¤ºä¾‹: ID={movies[0][0]}, URL={movies[0][1]}")
            return movies

        except Exception as e:
            logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []
        finally:
            session.close()
    
    def create_browser_with_tabs(self):
        """åˆ›å»ºæµè§ˆå™¨å¹¶æ‰“å¼€å¤šä¸ªæ ‡ç­¾é¡µ"""
        logger.info(f"ğŸš€ åˆ›å»ºæµè§ˆå™¨å¹¶å‡†å¤‡ {self.max_tabs} ä¸ªæ ‡ç­¾é¡µ")
        
        options = ChromiumOptions()
        options.headless(False)
        options.set_argument('--window-size=1920,1080')
        options.set_argument('--disable-blink-features=AutomationControlled')
        
        self.browser = ChromiumPage(addr_or_opts=options)
        
        # å»ºç«‹ä¼šè¯
        logger.info("ğŸ“± å»ºç«‹ä¼šè¯...")
        self.browser.get("https://missav.ai/")
        time.sleep(2)
        
        # åˆ›å»ºå¤šä¸ªæ ‡ç­¾é¡µ
        self.tabs = [self.browser]
        
        for i in range(self.max_tabs - 1):
            try:
                new_tab = self.browser.new_tab()
                self.tabs.append(new_tab)
                logger.info(f"âœ… åˆ›å»ºæ ‡ç­¾é¡µ {i+2}/{self.max_tabs}")
                time.sleep(0.5)
            except Exception as e:
                logger.warning(f"åˆ›å»ºæ ‡ç­¾é¡µ {i+2} å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¯ æˆåŠŸåˆ›å»º {len(self.tabs)} ä¸ªæ ‡ç­¾é¡µ")
        return len(self.tabs)
    
    def extract_movie_info_from_html(self, html, movie_id, movie_code, url):
        """ä½¿ç”¨parse_movie_pageæ–¹æ³•æå–ç”µå½±ä¿¡æ¯"""
        try:
            if HAS_DRISSION_CRAWLER:
                # ä½¿ç”¨MovieDetailCrawlerçš„parse_movie_pageæ–¹æ³•
                crawler = MovieDetailCrawler(movie_code)
                result = crawler.parse_movie_page(html)

                # æ·»åŠ æ•°æ®åº“ID
                result['id'] = movie_id
                result['timestamp'] = time.time()
                result['page_length'] = len(html)

                logger.info(f"ä½¿ç”¨parse_movie_pageæˆåŠŸæå–: {movie_code}")
                return result
            else:
                # ç®€åŒ–ç‰ˆæœ¬çš„HTMLè§£æ
                soup = BeautifulSoup(html, 'html.parser')

                info = {
                    'id': movie_id,
                    'code': movie_code,
                    'url': url,
                    'timestamp': time.time(),
                    'page_length': len(html)
                }

                # æå–æ ‡é¢˜
                title_tag = soup.find('h1')
                if title_tag:
                    info['title'] = title_tag.get_text().strip()
                else:
                    info['title'] = "æœªçŸ¥æ ‡é¢˜"

                # æå–å¥³ä¼˜ä¿¡æ¯
                actresses = []
                actress_links = soup.find_all('a', href=re.compile(r'/actresses/'))
                for link in actress_links:
                    actress_name = link.get_text().strip()
                    if actress_name and actress_name not in actresses:
                        actresses.append(actress_name)
                info['actresses'] = actresses[:5]  # æœ€å¤š5ä¸ª

                # æ£€æŸ¥M3U8å’Œç£åŠ›é“¾æ¥
                html_lower = html.lower()
                info['has_m3u8'] = 'm3u8' in html_lower
                info['magnet_count'] = html_lower.count('magnet:')

                # æå–ç£åŠ›é“¾æ¥
                magnet_links = re.findall(r'magnet:\?[^"\'>\s]+', html)
                info['magnet_links'] = magnet_links[:10]  # æœ€å¤š10ä¸ª

                logger.info(f"ä½¿ç”¨ç®€åŒ–è§£ææˆåŠŸæå–: {movie_code}")
                return info

        except Exception as e:
            logger.error(f"æå–ä¿¡æ¯å‡ºé”™: {e}")
            return None
    
    def crawl_movie_in_tab_with_retry(self, tab_index, movie_data):
        """åœ¨æŒ‡å®šæ ‡ç­¾é¡µä¸­çˆ¬å–ç”µå½±ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        movie_id, movie_url = movie_data
        tab = self.tabs[tab_index]

        # ä»URLæå–ç”µå½±ä»£ç 
        movie_code = movie_url.split('/')[-1] if '/' in movie_url else "unknown"

        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries):
            try:
                logger.info(f"ğŸ“ [æ ‡ç­¾é¡µ{tab_index+1}] å°è¯• {attempt+1}/{self.max_retries}: ID={movie_id}, {movie_code}")

                # è®¿é—®é¡µé¢
                tab.get(movie_url)

                # å¿«é€Ÿæ£€æŸ¥åŠ è½½çŠ¶æ€
                for check in range(3):
                    time.sleep(1)
                    html = tab.html
                    html_length = len(html) if html else 0

                    if html_length > 50000:
                        logger.info(f"âœ… [æ ‡ç­¾é¡µ{tab_index+1}] ID={movie_id} é¡µé¢å·²åŠ è½½ ({html_length} å­—ç¬¦)")
                        break

                # å¿«é€Ÿæ»šåŠ¨
                try:
                    tab.scroll(500)
                    time.sleep(0.3)
                    tab.scroll(0)
                except:
                    pass

                # æå–ä¿¡æ¯
                html = tab.html
                if html and len(html) > 10000:
                    movie_info = self.extract_movie_info_from_html(html, movie_id, movie_code, movie_url)

                    # éªŒè¯æå–ç»“æœ
                    if movie_info and movie_info.get('title') and movie_info.get('title') != "æœªçŸ¥æ ‡é¢˜":
                        # çº¿ç¨‹å®‰å…¨åœ°ä¿å­˜ç»“æœ
                        with self.lock:
                            self.results.append(movie_info)
                            self.save_single_result(movie_info)
                            logger.info(f"âœ… [æ ‡ç­¾é¡µ{tab_index+1}] ID={movie_id}: {movie_info.get('title', 'æœªçŸ¥')[:30]}...")
                            return True
                    else:
                        logger.warning(f"âš ï¸ [æ ‡ç­¾é¡µ{tab_index+1}] ID={movie_id}: ä¿¡æ¯æå–ä¸å®Œæ•´ï¼Œå°è¯•é‡è¯•")
                        if attempt < self.max_retries - 1:
                            time.sleep(2)  # é‡è¯•å‰ç­‰å¾…
                            continue
                else:
                    logger.warning(f"âš ï¸ [æ ‡ç­¾é¡µ{tab_index+1}] ID={movie_id}: é¡µé¢å†…å®¹ä¸è¶³ï¼Œå°è¯•é‡è¯•")
                    if attempt < self.max_retries - 1:
                        time.sleep(2)  # é‡è¯•å‰ç­‰å¾…
                        continue

            except Exception as e:
                logger.error(f"âŒ [æ ‡ç­¾é¡µ{tab_index+1}] ID={movie_id} å°è¯• {attempt+1} å¤±è´¥: {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(2)  # é‡è¯•å‰ç­‰å¾…
                    continue

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        with self.lock:
            self.failed_movies.append(movie_data)
        logger.error(f"ğŸ’€ [æ ‡ç­¾é¡µ{tab_index+1}] ID={movie_id}: 3æ¬¡é‡è¯•å‡å¤±è´¥ï¼Œè·³è¿‡")
        return False
    
    def save_single_result(self, movie_info):
        """ä¿å­˜å•ä¸ªç»“æœåˆ°JSONLæ–‡ä»¶"""
        try:
            with open(self.output_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(movie_info, ensure_ascii=False) + '\n')
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def parallel_crawl_batch(self, movie_batch):
        """å¹¶è¡Œçˆ¬å–ä¸€æ‰¹ç”µå½±"""
        logger.info(f"ğŸš€ å¹¶è¡Œå¤„ç† {len(movie_batch)} éƒ¨ç”µå½±")
        
        with ThreadPoolExecutor(max_workers=len(self.tabs)) as executor:
            futures = []
            
            for i, movie_data in enumerate(movie_batch):
                tab_index = i % len(self.tabs)
                future = executor.submit(self.crawl_movie_in_tab_with_retry, tab_index, movie_data)
                futures.append((future, movie_data, tab_index))

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future, movie_data, tab_index in futures:
                try:
                    success = future.result(timeout=60)  # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥é€‚åº”é‡è¯•
                except Exception as e:
                    movie_id = movie_data[0]
                    logger.error(f"âŒ [æ ‡ç­¾é¡µ{tab_index+1}] ID={movie_id} è¶…æ—¶æˆ–å‡ºé”™: {e}")
                    with self.lock:
                        self.failed_movies.append(movie_data)
    
    def run_database_crawl(self, limit=None):
        """è¿è¡Œæ•°æ®åº“çˆ¬å–"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“å¹¶è¡Œçˆ¬å–")
        
        # è·å–æ–­ç‚¹
        start_id = self.get_last_processed_id()
        
        # è·å–ç”µå½±åˆ—è¡¨
        movies = self.get_movies_from_database(start_id, limit)
        if not movies:
            logger.info("ğŸ“­ æ²¡æœ‰æ‰¾åˆ°å¾…å¤„ç†çš„ç”µå½±")
            return
        
        # åˆ›å»ºæµè§ˆå™¨
        actual_tabs = self.create_browser_with_tabs()
        if actual_tabs == 0:
            logger.error("âŒ æ— æ³•åˆ›å»ºæ ‡ç­¾é¡µ")
            return
        
        logger.info(f"ğŸ“Š å‡†å¤‡å¤„ç† {len(movies)} éƒ¨ç”µå½±ï¼Œæ‰¹æ¬¡å¤§å°: {self.batch_size}")
        
        start_time = time.time()
        
        try:
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(movies), self.batch_size):
                batch = movies[i:i + self.batch_size]
                batch_num = i // self.batch_size + 1
                total_batches = (len(movies) + self.batch_size - 1) // self.batch_size
                
                logger.info(f"\nğŸ¬ æ‰¹æ¬¡ {batch_num}/{total_batches}: {len(batch)} éƒ¨ç”µå½±")
                logger.info(f"ğŸ“‹ IDèŒƒå›´: {batch[0][0]} - {batch[-1][0]}")
                
                # å¹¶è¡Œå¤„ç†è¿™ä¸€æ‰¹
                self.parallel_crawl_batch(batch)
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if i + self.batch_size < len(movies):
                    rest_time = random.uniform(3, 5)
                    logger.info(f"ğŸ˜´ æ‰¹æ¬¡é—´ä¼‘æ¯ {rest_time:.1f} ç§’...")
                    time.sleep(rest_time)
                
                # æ˜¾ç¤ºè¿›åº¦
                elapsed = time.time() - start_time
                processed = min(i + self.batch_size, len(movies))
                avg_time = elapsed / processed
                remaining = (len(movies) - processed) * avg_time
                
                logger.info(f"ğŸ“Š è¿›åº¦: {processed}/{len(movies)}, "
                          f"å¹³å‡ {avg_time:.1f}ç§’/éƒ¨, é¢„è®¡å‰©ä½™ {remaining/60:.1f}åˆ†é’Ÿ")
        
        finally:
            # å…³é—­æµè§ˆå™¨
            if self.browser:
                try:
                    self.browser.quit()
                    logger.info("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")
                except:
                    pass
        
        # è¾“å‡ºç»Ÿè®¡
        total_time = time.time() - start_time
        success_count = len(self.results)
        failed_count = len(self.failed_movies)
        
        logger.info(f"\n{'='*50}")
        logger.info(f"ğŸ“Š æ•°æ®åº“çˆ¬å–å®Œæˆ")
        logger.info(f"æ€»æ•°: {len(movies)}")
        logger.info(f"æˆåŠŸ: {success_count}")
        logger.info(f"å¤±è´¥: {failed_count}")
        logger.info(f"æˆåŠŸç‡: {success_count/len(movies)*100:.1f}%")
        logger.info(f"æ€»æ—¶é—´: {total_time/60:.1f} åˆ†é’Ÿ")
        logger.info(f"å¹³å‡é€Ÿåº¦: {total_time/len(movies):.1f} ç§’/éƒ¨")
        logger.info(f"è¾“å‡ºæ–‡ä»¶: {self.output_file}")

def main():
    """ä¸»å‡½æ•°"""
    
    logger.info("ğŸš€ æ•°æ®åº“å¹¶è¡Œçˆ¬è™«")
    logger.info("ğŸ“Š åŠŸèƒ½ç‰¹æ€§:")
    logger.info("  - ä»æ•°æ®åº“æŒ‰IDé¡ºåºå¤„ç†")
    logger.info("  - 5ä¸ªæ ‡ç­¾é¡µå¹¶è¡Œ")
    logger.info("  - æ‰¹æ¬¡å¤§å°: 5éƒ¨ç”µå½±")
    logger.info("  - æ‰¹æ¬¡é—´éš”: 3-5ç§’")
    logger.info("  - æ”¯æŒæ–­ç‚¹ç»§ç»­")
    logger.info("  - ä½¿ç”¨parse_movie_pageæ–¹æ³•æå–")
    logger.info("  - å¤±è´¥é‡è¯•3æ¬¡")
    logger.info("  - è¾“å‡ºåˆ°JSONLæ–‡ä»¶")
    
    # è¯¢é—®å¤„ç†æ•°é‡
    limit_input = input("\nğŸ”¢ é™åˆ¶å¤„ç†æ•°é‡ (å›è½¦=ä¸é™åˆ¶): ").strip()
    limit = int(limit_input) if limit_input.isdigit() else None
    
    if limit:
        logger.info(f"ğŸ“Š å°†å¤„ç†æœ€å¤š {limit} éƒ¨ç”µå½±")
    else:
        logger.info("ğŸ“Š å°†å¤„ç†æ‰€æœ‰å¾…å¤„ç†ç”µå½±")
    
    # ç¡®è®¤å¼€å§‹
    start = input(f"\nğŸš€ å¼€å§‹æ•°æ®åº“çˆ¬å–? [y/n]: ").lower()
    if start != 'y':
        logger.info("ğŸ‘‹ ä¸‹æ¬¡è§ï¼")
        return
    
    # åˆ›å»ºçˆ¬è™«å¹¶å¼€å§‹
    crawler = DatabaseParallelCrawler(max_tabs=5, batch_size=5)
    crawler.run_database_crawl(limit=limit)

if __name__ == "__main__":
    main()
